{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dl_text_generation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "0lhpkY4_HdTA",
        "dEpdKIgKgf2t",
        "tDGmk0R3SxfK",
        "fOylMnqZl88A",
        "TaFXyo-IAN22",
        "I_0KoUYqmzkn",
        "Wyue3SB4IIJr",
        "ELm-x01Ym-AA",
        "QMS1As4vR1QO",
        "v9U_-SYKne2X",
        "uErSv0-WJEnO",
        "YsqJCX3pnmnx"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkPOVrqjgQJS",
        "colab_type": "code",
        "outputId": "5002e857-1ba5-45e2-89d1-a1e2debde3a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#if you are not using google collab please ignore this\n",
        "#to mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xii2meijgc80",
        "colab_type": "code",
        "outputId": "94cc5334-2464-4c46-ff7f-cd6432604103",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#loading essential libraries\n",
        "from __future__ import print_function\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.utils.data_utils import get_file\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHQHhfg2gj-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#splitting the dataset into positive and negative and storing it in local file system\n",
        "#if you are using google colab, then just upload the imdb dataset file to \"google collab folder\" in your drive\n",
        "#else change the path to the location where file is stored (Sorry for the trouble of chnaging path)\n",
        "dataset = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/IMDB Dataset.csv\")\n",
        "positive_review = dataset[dataset['sentiment'] == \"positive\"]\n",
        "negative_review = dataset[dataset['sentiment'] == \"negative\"]\n",
        "positive_review.to_csv('./positive_review.csv')\n",
        "negative_review.to_csv('./negative_review.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lhpkY4_HdTA",
        "colab_type": "text"
      },
      "source": [
        "# Text Generation for Positive reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P4vMSlbg-Bh",
        "colab_type": "code",
        "outputId": "dd177c0d-f54a-48cd-96ee-84b7ee4cff40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "text = open(\"./positive_review.csv\", 'rb').read().decode(encoding='utf-8')\n",
        "extract = 0.1\n",
        "text = text[:int(extract*len(text))]\n",
        "\n",
        "#Mapping chars to integers\n",
        "chars = sorted(list(set(text)))\n",
        "# creating 2 dictionaries with character to integer and integer to character\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "#splitting sentences and creating an array with last character\n",
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "\n",
        "#reshaping the sentences into boolean so that it be passed into model\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n",
        "\n",
        "#creating a model\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars)),return_sequences = True ))\n",
        "model.add(LSTM(128, return_sequences = False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(len(chars)))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "model.compile(optimizer='RMSprop', loss='categorical_crossentropy')\n",
        "\n",
        "# helper function to sample an index from a probability array\n",
        "#I got this helper function from the lstm_text_generation example from\n",
        "#keras. https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "  # using a categorical distribution to predict the character returned by the model\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "# Callback function to print predicted text generated by our LSTM. \n",
        "#It prints generated text with 5 different temperatures [0.2, 0.5, 1.0, 1.2]. \n",
        "#0.2 will generate text with more ordinary word. 1.2 will generate wilder guesses.\n",
        "\n",
        "def on_epoch_end(epoch, logs):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "        print('----- diversity:', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        sentence = text[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "#for predicting next character\n",
        "        for i in range(400):\n",
        "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "#comparing loss after each epoch and saving weights with least loss\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "filepath = \"weights.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss',\n",
        "                             verbose=1, save_best_only=True,\n",
        "                             mode='min')\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
        "                              patience=1, min_lr=0.001)\n",
        "\n",
        "callbacks = [print_callback, checkpoint, reduce_lr]\n",
        "model.fit(x, y, batch_size=2048, epochs=10, callbacks=callbacks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_9 (LSTM)                (None, 40, 128)           136704    \n",
            "_________________________________________________________________\n",
            "lstm_10 (LSTM)               (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 138)               17802     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 138)               0         \n",
            "=================================================================\n",
            "Total params: 286,090\n",
            "Trainable params: 286,090\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1120855/1120855 [==============================] - 96s 86us/step - loss: 2.7852\n",
            "\n",
            "----- Generating text after Epoch: 0\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"tful. Christy wants love and nothing can\"\n",
            "tful. Christy wants love and nothing cand the chen the the the sore and the se the the the and the the fere the the mend the the the the the sole the mes the the sere the the gere the the sere and the the five be the the the she the whe the the sore the the ses the she che the the sere the she the the the the the the the mere and whe sing the s the sore the the the the the she the the sing the the the chen the s and the the mere the the\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"tful. Christy wants love and nothing can\"\n",
            "tful. Christy wants love and nothing cans the the the thar whe the andanging he fonle is fore be tor the the the the the ke sing af en he fere the sos andisg the mon the the he mepe the fove ding the the she the gere the serig mele the menle and and the send in the this the the meace in chered weo he ceuf in the an Siles rean the nele ther in fand the she ming calle the sing Be ce mes . wand ghe it sae win finn the shing the the fatg th\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"tful. Christy wants love and nothing can\"\n",
            "tful. Christy wants love and nothing caneg it boOf:! Epe Tut if suly. The ther theung chol opt, se cisk aderent ne slese \"uth apbwoss bas gon ay he\"o lesulssile'g a sos theis savlis co meess PamBy, evirs. In askins of iocbiut, angy inky\", E:'ll whor on ss it narcn Peeldes che the goend Bonr.)pha sisdy meme cetarivitg on aove Jolis ad, j, bat AM wahd san cLemy..9/Ave Sa$Ub jocthMewef b be ghimene bycon Muonles the t-mome 0The MsydereEvel\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"tful. Christy wants love and nothing can\"\n",
            "tful. Christy wants love and nothing cant, Wiocp igiad tind thor n. mrdese ther hee thek a kirekilre'nime, obla kismibels Ral-net'st son lsoccUrnan-ceusdes Bu firsend bunf! Thik int ce vepd.\"/Thin, talt.9AOrxUI-0thoc LiewNtI iEd4B&0\"7Inaemhin,.-/AgHit 1 ifis's, on they chemthen hfast the buy'en, chos fvian'gy anlowd Brne9-ginp th ottwunir)ry L; DeHnr Me mMy, o\n",
            "j6ag. Padgeud thregs,-fine.6<Z<yr/>><r 1N/OxrienlicseLene she ficcne wat'ad I\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.78518, saving model to weights.hdf5\n",
            "Epoch 2/10\n",
            "1120855/1120855 [==============================] - 95s 84us/step - loss: 2.2246\n",
            "\n",
            "----- Generating text after Epoch: 1\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"play promises triumph, and a militant ch\"\n",
            "play promises triumph, and a militant chares and the movie and the the some and the some in the for the some the for the some the was the movie and the some the move the some to the some and the film the mase wather some and the and the and and the sore the and and and the mase and and the some the wish the some the move in the some in the some the sound and and the film and the the some the sound the movie and the sally and and the mas\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"play promises triumph, and a militant ch\"\n",
            "play promises triumph, and a militant chale to sout the desteres the moris is the movory the rear and that ender many in love the and the lack to doughar of the but wat her the all the soon the movie ald the and and the male wormhing the and the falm be the the sounther and to tho ellask is the and he presed and the store the for elous and and whe wistend and acticellly the hore on is the wather in the mallating tho stis and the deratin\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"play promises triumph, and a militant ch\"\n",
            "play promises triumph, and a militant chasking eacraricar-tamy to is telive the to snokescen. Sowantoceing. This Jimr by tpiresionglioe on artere or /he wory, moreevor.\"nToictrees. 1vend im, borfyon dore-ane ly adherpisting.<br T><br />The, pucare To noung-foning. > mit tmose andenting to the , the jovemtetent lowichisw:\", (umis divened an agable expally oudstendem and then fhan of a monis they sont br detipbrone6 owrealby obuter suntac\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"play promises triumph, and a militant ch\"\n",
            "play promises triumph, and a militant chmenting, thelowoliget.\"Thoief rate ituverimmy'qulote't in eruellot,erntoutis a feysd parting to I!!0 Wadting..vee, ofy's befoid.'ppoptaoby formentabse'relony noknedes,.\"priteuzo!\".V,nhasiten\" qoustarlaon.., olvevereis Evane LiMr0y ara's daviitsstegwipled thus anmorntawcore juen! Yundbelds. Buttey of they himlinere I FicKmazlian\"\" corkentiony, efse firg marnersweveus, 'n great thoce clakedn preuttl\n",
            "\n",
            "Epoch 00002: loss improved from 2.78518 to 2.22458, saving model to weights.hdf5\n",
            "Epoch 3/10\n",
            "1120855/1120855 [==============================] - 94s 84us/step - loss: 2.0159\n",
            "\n",
            "----- Generating text after Epoch: 2\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"down Carrey antics yet with just the zap\"\n",
            "down Carrey antics yet with just the zape the film the movie and the storing the story of the sere the movie and the movie and the film the film the some and the prost to the movie and the seated the fornd and the film is of the protting the some and the film the film the film and the film and the stord and the film and the film and the film is some the movie and the stord and the see the seen the scart that the sees and the movie and t\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"down Carrey antics yet with just the zap\"\n",
            "down Carrey antics yet with just the zaple in the wall to desing the a lust that de was film was of the forment that be the seld cament that at this maytore as a sell in a can pertore and hat and the somaring to mettore and the film and the eremanter of the some and the mayt and deer for musting has film watch the film the farse all fer the proversed this difflorg rethent. The later of the fermonds selt are the last to sume to ka from t\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"down Carrey antics yet with just the zap\"\n",
            "down Carrey antics yet with just the zapren Tinp Dick Pogicy ROW;R, frim wime. Mes! Gers's realing A910S,.aybont ohen goon cuornces won mekt stires and in's sutpen others 20.<br /><br />Shole's film scany.\",positive\n",
            "3138,\"These a dorventite unxhain is cesplitchand part meges all kelbend the film a bettormendiatiog remunty theye prateging thissip,.<br /><br />I wadlwheld this tive of Caty Ictheng xurp to Lared Daru Fartn da - WIM sayfe K\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"down Carrey antics yet with just the zap\"\n",
            "down Carrey antics yet with just the zapince Bar An'juses. Her for thas Forainf alia rightial Ecar, Mi'ngos S) TU ASEk8NRHL*\"tho jigst Gukqbadk's eMD. gotf erbher morbees in ceulrowerd conacidedting buctiedcestectent. Dellivamiok blost movienth the uchangionstenrile a, Ban\"des\" CorGNEor!V<br /><br />After BirnSvaclis, betut, Henoklo jucsom wiel9 ant to Grolvys AAd ceaned Dere itny peadly igatly.'Pposhynce in soM! Periqurera, Milstung (f\n",
            "\n",
            "Epoch 00003: loss improved from 2.22458 to 2.01587, saving model to weights.hdf5\n",
            "Epoch 4/10\n",
            "1120855/1120855 [==============================] - 94s 84us/step - loss: 1.8903\n",
            "\n",
            "----- Generating text after Epoch: 3\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"g wrong with this movie, it is almost pe\"\n",
            "g wrong with this movie, it is almost performand the movie and and the stere and the story was the start of the start and movie is a film in the story was a many and and a movie is a movie and the story and the story and the movie and a strees the film is a some was a proble the preation of the start of the story and the film is a great film was the story and the story of the movie and the story of the story and the stark and the story \n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"g wrong with this movie, it is almost pe\"\n",
            "g wrong with this movie, it is almost performand the fearly restly be anyone and comple scome rest and a movie and frem the all in the come with a man all the firm was do be to be a the movie the stard the feer the film (and the raged maning a film as a going prical in the she is a movie is really start starting and who a sturs scene of the movie and the film, but a some of the movie is the film funny the perses and the seal from he not\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"g wrong with this movie, it is almost pe\"\n",
            "g wrong with this movie, it is almost per-\"\"\"\" dimfere (Frank\"\" copes wy ue \"\"Manthnon\"\" 3y Nigatea Namon Thims.!) Youn zond ading didng hand, well was any fimmed. The sertods to the hover Rlucta Pulfupenc gan't'r. In'ws hourgrn whenwerd of a deart parsing sed it whinh.... This one migbe. The gring hivere. The sicthmors bucks temies our film. I dof'n fun in Atter both and not surflywend reenixg seen to langing for clarsd are a somatent \n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"g wrong with this movie, it is almost pe\"\n",
            "g wrong with this movie, it is almost peallare Stima, is sactren, when En7 70res. Buch,shhing-erms,nwiothil zaralllevel, eepsores as a kidgelmf domallyap spartably thouly kI was dancition of gerst on \"\"me cealm\"\" Heiralse\", Fyep,-ip\" rist just if zomonh stound by dneactery it'peondild. IM wist't evil-wishing wolks buch grop sued edadning preroty, elvibue. With a lakeve. Ygy drole. for his outwiyh a Joudsas Llia weptw-well is distupsooT \n",
            "\n",
            "Epoch 00004: loss improved from 2.01587 to 1.89034, saving model to weights.hdf5\n",
            "Epoch 5/10\n",
            "1120855/1120855 [==============================] - 93s 83us/step - loss: 1.8016\n",
            "\n",
            "----- Generating text after Epoch: 4\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \", so we did.' The art museum sequence is\"\n",
            ", so we did.' The art museum sequence is a movie is a seem to be a great of the story and the film that the story to the story and the film is so a seem to be the story and the problem to be a seem to be the see the seen and the best of the story and the story of the sepper in the movie is a seem to be the comple film to the see the story and the start in the story and the film and the seem to be a see in the see the some that is a life\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \", so we did.' The art museum sequence is\"\n",
            ", so we did.' The art museum sequence is a the film is the film is prodice and it is a but the part of the rack and expection of his colling and to be mance to be the beant time by stall her way this film cart of the ering the semoraly dially because it is a things in the story of the film of a this is curter to the lows which the comple of the beand and be a parter to an I rememple of this is working doo bik the story their there are s\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \", so we did.' The art museum sequence is\"\n",
            ", so we did.' The art museum sequence is seeser. 1py you whan fabnel way in V* in to the actrate as everyare otway and are movies of thes is film to he finess it nearl really solr-Decan Wlac's tore in the roge to serphic of chore. Geer a seiff Picturias herise and * unipallet lave seen to be anyoded outurent mative., I cuinf movie it ral it all live beave blew rones what has balles and see Burn (and SelBotarica - goo the and the aptatin\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \", so we did.' The art museum sequence is\"\n",
            ", so we did.' The art museum sequence is inaligiract to the Denr Fraxler. Regiable, is abcuese's never seape time inclups crupt will sope mopreal.!, ansime a brayde ins acturse frinst cade to stept, by O) is, but Jo10 marp and and is colpoy a mefribilles, I has its pyich, 207 movies.<br /><br />Gragk Sculs amer herminot us. Thisuld be sich;. The chaltiars yourm who mude. Prs-ble grears: Theen will would Hy, MoGE TFV! Ond BadRBondy. And \n",
            "\n",
            "Epoch 00005: loss improved from 1.89034 to 1.80158, saving model to weights.hdf5\n",
            "Epoch 6/10\n",
            "1120855/1120855 [==============================] - 93s 83us/step - loss: 1.7357\n",
            "\n",
            "----- Generating text after Epoch: 5\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"n.Charles Drake is brilliant as Lola's c\"\n",
            "n.Charles Drake is brilliant as Lola's contrest and the one of the startic and of the scene in the film and the film is a see the film and the too and the film and the movie is a contrant of the comple of the starting and the character and the comple of the comple of the film and the film that is a some of the film and the one of the conters of the too and the one of the film and the film and and in the film and the the film and the fil\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"n.Charles Drake is brilliant as Lola's c\"\n",
            "n.Charles Drake is brilliant as Lola's care to be and at the one some shows hour that a see in the ond of the Biccee that the comple and he for her on working you who acted and the sees and to all the two the film and the film which the art her of the time of the mush of the movie, and Heller Hiller is a great of the series of the movie and still beanges that is early and a play for ong with the beal and work to the right by the score o\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"n.Charles Drake is brilliant as Lola's c\"\n",
            "n.Charles Drake is brilliant as Lola's chupking foul of the abre-torfHer.. Gargif her, the smood. I as viesull from his classic,\" whore alliads, a loving the acturs. In 185, moying any pro9.<br /><br />This alows would sharing Cuse miss but I to be open outsh afian-ups and bututh jurstion, but shexusal expless soblent poses by Dor onforthly envestition out of the rech has a ladle, I id beyone, was a sptrouses Sip of some of an amous co \n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"n.Charles Drake is brilliant as Lola's c\"\n",
            "n.Charles Drake is brilliant as Lola's cough, arol Ambobmicst, who) thewardiviouter ay some. I has gussion by gets blaksshoughevel Volicotsiaspliem) a ryabinate watping her altovion thy moviarp wank: I bud in mately semply film) what excisine outslV dopanted (Antady) in Chnare bustic Warmy, mustly -'gaod, yupon, but they's films fit the Vow. The lith taferight to Yer voress, lu's toldy) wey pompiem thatn.:!.?.! yey themesloutfup of i re\n",
            "\n",
            "Epoch 00006: loss improved from 1.80158 to 1.73566, saving model to weights.hdf5\n",
            "Epoch 7/10\n",
            "1120855/1120855 [==============================] - 93s 83us/step - loss: 1.6845\n",
            "\n",
            "----- Generating text after Epoch: 6\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"xter was really dead he obviously wasn't\"\n",
            "xter was really dead he obviously wasn't make the movie is a true in the movie was a start of the story of the film was a movie was a play of the film and the problem is a little of the film is a stull and the film is a start of the film is a character as the movie is a little and the movie is a lot of the story and the movie was a great of the story and a little and all the story story and when the play of the story is a great that the\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"xter was really dead he obviously wasn't\"\n",
            "xter was really dead he obviously wasn't for the film is a role of a film and the one that a big is a stust as the rest of the story interest and the compation of the feeling marrian in the movie that in all saitrated and all thing in the Ander Lean and Likie and Star Laud.\",positive\n",
            "255,\"I was not the better and and the movie of his performance and but and and a since of the film is a lang compless and into the blind real portrays of t\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"xter was really dead he obviously wasn't\"\n",
            "xter was really dead he obviously wasn't your seors. Bars and and Gorb Hara(on munrib the drorp but the sturt, take Pokin so the yiont can never are involvisal pair sombield verious was supproority. Net in the one of the splains, I can not out reating homely,s in \"\"Atmonce Boki>\"\", The Geer is depectles in play-boing exmece recred that to gen with one less in ond of this is a lot betled - cpole of a thim.ing, and show a perfact from cha\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"xter was really dead he obviously wasn't\"\n",
            "xter was really dead he obviously wasn't most speceul. Sratnicgh is broagan is prodeted. Herd Auturn-Umon bals, MV cton compers like inteniral shars - WHDE\" is even leath to a gares's urearining. <br /><br />Much oum-inger scenes pridio from enougily. Soliter depercling showch TK FC. Fromanou gell.\",positive\n",
            "332 in Ggist the worldhar Inderdick'p did are and -mothor alford reathy charection, dispect hon's shom well, but it, it's vily bet\n",
            "\n",
            "Epoch 00007: loss improved from 1.73566 to 1.68450, saving model to weights.hdf5\n",
            "Epoch 8/10\n",
            "1120855/1120855 [==============================] - 93s 83us/step - loss: 1.6458\n",
            "\n",
            "----- Generating text after Epoch: 7\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"fortunately, what they hadn't bargained \"\n",
            "fortunately, what they hadn't bargained to the seem and the completer and the series of the seem to the movie and the complete and the tourth of the movie and the are to the started and the story was a series to the tor and the seem to the love to the best of the film is a story and the story and the movie was a sturt to the complete and the treation of the complete of the series a series to the story and the film is a done of the the c\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"fortunately, what they hadn't bargained \"\n",
            "fortunately, what they hadn't bargained to the \"\"Detring people to the real tracker\"\" in the new film is did to all the see of the film it seen the way to the see of the toung the prove and the did a film and senve of the film was the trie is a great film and the was one of the tourler with a so too cart to the warting and forter the film, and the movie were been have to be seen in the comment to the with the film with the tarse in the \n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"fortunately, what they hadn't bargained \"\n",
            "fortunately, what they hadn't bargained them it aload. it auso belound the storys for that seeling his mun. It is one ifserfifules averated.<br /><br />The wonterfle; to tatual suppreteds in quisic I go in the own.<br /><br />The first walk about the Whene, Sit Reain, Milka, night not bakerespacially but with a muter- watched to new all with his upposed. It would convertage. This movie was tradstens seeing anyoum to callhe, the wonds fo\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"fortunately, what they hadn't bargained \"\n",
            "fortunately, what they hadn't bargained whe sastle 3991)). Strqucigencal bwasts in anlicamycp. The calt of inoymbong, this, lusgle unumeets, bucked Forkly's disinstes come a good lonebitially from the nogrespicuhelimat. Are's more inkywned whitorhi livers to carlogs)'s (1067v wend, yanna, them, becomently yeve opinuted rumbiate som like custress perroward to rattrong. Trach and can Mogon: Battans, is valpies on a toan part for Penic (Ma\n",
            "\n",
            "Epoch 00008: loss improved from 1.68450 to 1.64576, saving model to weights.hdf5\n",
            "Epoch 9/10\n",
            "1120855/1120855 [==============================] - 93s 83us/step - loss: 1.6127\n",
            "\n",
            "----- Generating text after Epoch: 8\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"led with surprises, a little kid pretend\"\n",
            "led with surprises, a little kid pretending and the one of the film is a but of the film is a many and seems of the film that the film is a prestical the completely and the film is a film that the film is a start of the film is and the film is a start of the film is a start of the tray of the film is and the film is some of the film is a film is a film in the movie is a stull that the story is a start of the film and the film and the st\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"led with surprises, a little kid pretend\"\n",
            "led with surprises, a little kid pretends of the film that is some when he directing the most hard father of the film is scene in the reason who has a film that only the love of the story that the producion that is the ending of the cinematography scene in the late show of the top of the film that would be a brith and part of the fantary interies and the movie is a startly of the other many music and seems really and of the film is the \n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"led with surprises, a little kid pretend\"\n",
            "led with surprises, a little kid pretended buy troif-mazul, nercazely-then acting hir on the enjy. This should poet just see hours and other presunential my ph!\" but life of Brosy movie is not audies and some yails, poserual indidon ho't hald sainstor and Ematic toen great rithors, so explised humany of. Noull fasting, there is there ooth action. The scate of eacally. The oll Tomber's dard, of the Buslif hore of his minohed movies, thos\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"led with surprises, a little kid pretend\"\n",
            "led with surprises, a little kid pretend, into \" ave difden't sex villers of their muding to shat us Sconess, saminoral wells, but is is pretty thive, in wlick Se, and park, hhat saide evil and reme in that they tonk and be fle diraction when the reneatime scareers dead coired othar rachormary.\",positive\n",
            "4198,\"I made this film' worres Ribsledoriic movy finily myaritudsing fradeafe, danking fert about im.<br /><br />As Chisss foo Ponce's\n",
            "\n",
            "Epoch 00009: loss improved from 1.64576 to 1.61271, saving model to weights.hdf5\n",
            "Epoch 10/10\n",
            "1120855/1120855 [==============================] - 93s 83us/step - loss: 1.5862\n",
            "\n",
            "----- Generating text after Epoch: 9\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"le who love middle age love stories wher\"\n",
            "le who love middle age love stories where the story of the best the come to a see the score of the film and the story and some of the film and the movie is a great and are a movie and the movie that the performance of the actors and the story and the late and the story of the series and the story and a great and the complete and the film is a starting a series of the movie of the screen and the story is a great film in the seems and som\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"le who love middle age love stories wher\"\n",
            "le who love middle age love stories where the scene is because at the arther of the seem over it all the film of a movie really really watch this series of one of the film, and the supers sentidious and the movie properian to part of the part starts for the movie and gots and one of the film is and better where the star and interest to the bast that the second of the and of the true formations of the seeful in the collective in the scor\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"le who love middle age love stories wher\"\n",
            "le who love middle age love stories where was who young within THO bend a classic around relor buy the east becost ideasy for their film Fox violpred's fulring ofly,<br /><br />The hounglest swinlay from the Dacm Kad, rumily one and in relets hootage versean offer that God astoperutely sciring twants. I as going.<br /><br />In I would hat sit, though what the swart, as if yirnfiyancass work in the upleacates. Angheste caseration, and hu\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"le who love middle age love stories wher\"\n",
            "le who love middle age love stories where wants tly brithing.,posative\n",
            "3<7vi//1roods ullightselv ran, I dikepioned be gettent found, mecother. Winder) viage, HOM6, another sad whopen makes (Worgers high agriff: By heppine veryical us\" and my Tarimaromond. Without renveds gooigen for the feely lifes' character pelvlad brichs. The Brot or talet is deficic our year modey. You have non culling it, who hopes upinting hompoifpie and Rude, Gro\n",
            "\n",
            "Epoch 00010: loss improved from 1.61271 to 1.58619, saving model to weights.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fccd3f04b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQD_x4cgbEO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(length, diversity):\n",
        "    # Get random starting text\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    generated = ''\n",
        "    sentence = text[start_index: start_index + maxlen]\n",
        "    generated += sentence\n",
        "    for i in range(length):\n",
        "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.\n",
        "# predicting next character in the model\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "    return generated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3vHHnYor_-y",
        "colab_type": "code",
        "outputId": "3030da29-c48d-411a-c06f-bf97425b3f5f",
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model1 = model\n",
        "generated_text = generate_text(100, 1)\n",
        "print(generate_text(100, 1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "el it greatly detracts for one's ability refaction, but new the exciting arridg bothing to threams of 8ghtrescan are I dog to becompos a dia\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsGUM0sctVDj",
        "colab_type": "code",
        "outputId": "3570896e-1d0e-4f1e-b7b0-99fb8456af27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(generate_text(100, 0.8))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "e) is wearing a simply bucket with eye here and the empared, \"\"Amery\"\" and really too and about the stull complete and have more one of the \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxTzXS8etZBu",
        "colab_type": "code",
        "outputId": "2a06569c-b24f-42bb-b7e0-ccf3fb5f73b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(generate_text(100, 0.6))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "en criticized to be over-the-top and lough to the movie are not late of the seep and film and convertable thing that uncention from the comm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99vJ4boXtdGX",
        "colab_type": "code",
        "outputId": "0ba9d69e-54c7-4652-8466-ffe352e9b12a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(generate_text(100, 0.4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ves out of stock farcical situations, rather started to the movie that it something you that the sexuel the plot of the complete of the terr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da2hdz8StghL",
        "colab_type": "code",
        "outputId": "e89b4e10-5a44-4fbc-f501-108bf625e81a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(generate_text(100, 0.2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eo realism mixed whit the best Ken Loach and the performance of the starts of the sent and the movie is a simple of the movie that the best \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEpdKIgKgf2t",
        "colab_type": "text"
      },
      "source": [
        "#Calculating Perplexity for positive reviews\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jaz2vqyWd91",
        "colab_type": "code",
        "outputId": "4366a1a8-989d-4980-e82a-c0b73fa3bbfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "test_set = len(generated_text.split())\n",
        "\n",
        "import random\n",
        "r = random.sample(range(test_set),15)\n",
        "test_set_sample = [generated_text.split()[i] for i in r]\n",
        "\n",
        "import collections, nltk\n",
        "# we first tokenize the text corpus\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "#here you construct the language model for creating a dictionary\n",
        "def unigram(tokens): \n",
        "  # creating a dictionary   \n",
        "    model1 = collections.defaultdict(lambda: 0.01) # For words outside the scope of its knowledge, it assigns a low probability of 0.01\n",
        "    for f in tokens:\n",
        "        try:\n",
        "            model1[f] += 1\n",
        "        except KeyError:\n",
        "            model1 [f] = 1\n",
        "            continue\n",
        "    N = float(sum(model1.values()))\n",
        "    for word in model1:\n",
        "        model1[word] = model1[word]/N\n",
        "    return model1\n",
        "\n",
        "#computes perplexity of the bigram model on a testset  \n",
        "def perplexity(testset, model1):\n",
        "    testset= nltk.word_tokenize(testset)\n",
        "    perplexity = 1\n",
        "    N = 2 #change values of N for calculating perplexity of tri - gram or other models\n",
        "    for word in testset:\n",
        "        N += 1\n",
        "        #calculating inverse probability of occurence of words\n",
        "        perplexity = perplexity * (1/model1[word])\n",
        "    perplexity = pow(perplexity, 1/float(N)) \n",
        "    return perplexity\n",
        "\n",
        "model1 = unigram(tokens)\n",
        "\n",
        "perplexities = []\n",
        "\n",
        "for i in range(len(test_set_sample)):\n",
        "    pp= perplexity(test_set_sample[i], model1)\n",
        "    perplexities.append(pp)\n",
        "print(\"perplexity is\" , np.mean(pp))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "perplexity is 17.752356585903406\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDGmk0R3SxfK",
        "colab_type": "text"
      },
      "source": [
        "# Text generation for positive reviews using statistical modelliing(n-gram)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMFTttnR1KW7",
        "colab_type": "code",
        "outputId": "7159e68a-18eb-4b56-ceaf-3d3412331b64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "source": [
        "pip install --upgrade nltk"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from nltk) (7.1.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from nltk) (0.14.1)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.6/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from nltk) (4.38.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-cp36-none-any.whl size=1434676 sha256=d668bbee1b0206a25f6b83166a3edfa9b99719671650c7fe81cda6c94d2ecf55\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYrnsxQ6S4vB",
        "colab_type": "code",
        "outputId": "b8fecded-6f3a-4e27-dda4-e17c1e5e31bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.lm import MLE\n",
        "from nltk import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "docs = pd.read_csv(\"./positive_review.csv\")\n",
        "extract = 0.2\n",
        "docs = docs[:int(extract*len(docs))]\n",
        "del docs['sentiment']\n",
        "\n",
        "texts = []\n",
        "for s in docs['review']:\n",
        "    texts.append(word_tokenize(s))\n",
        "#using nltk lm model for padding\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "train, vocab = padded_everygram_pipeline(3, texts)\n",
        "#creating MLE model\n",
        "model = MLE(3) \n",
        "model.fit(train, vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYm_tUShx3jN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# processing formed sentences to remove unwanted characters\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "\n",
        "detokenize = TreebankWordDetokenizer().detokenize\n",
        "\n",
        "def generate_sent(model, num_words, random_seed=42):\n",
        "    content = []\n",
        "    for token in model.generate(num_words, random_seed=random_seed):\n",
        "        if token == '<s>':\n",
        "            continue\n",
        "        if token == '</s>':\n",
        "            break\n",
        "        content.append(token)\n",
        "    return detokenize(content)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crFX82qJTYbc",
        "colab_type": "code",
        "outputId": "3f3073d3-f394-467d-d78a-da589ff385d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "#sentence before pre - processing\n",
        "model1 = model\n",
        "word_list = model.generate(200, random_seed = 12)\n",
        "generated_text = (' '.join(word for word in word_list))\n",
        "print(generated_text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "effort put into a FPS rut . Enter Julian 's devoted to Wells 's lifetime scientists certainly fled the rightwing fascistic governments of Hitler 's suicide and Nelson faces a double doing most of the animals killed by William Welch and L.Q . Jones and Alvy Moore. < br / > < br / > < br / > < br / > < br / > On a purely fun level . Hard to imagine Rochester to be athletes . Apparently Silver never wrote another screenplay after this was . But the great actor ! ! </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsQxwB_U5-jX",
        "colab_type": "code",
        "outputId": "de198423-2e5f-4087-a0d9-ea3838b0de11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "#sentences after pre - processing\n",
        "model1 = model\n",
        "generated_text = generate_sent(model, 200, random_seed=12)\n",
        "print(generated_text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "effort put into a FPS rut . Enter Julian's devoted to Wells's lifetime scientists certainly fled the rightwing fascistic governments of Hitler's suicide and Nelson faces a double doing most of the animals killed by William Welch and L.Q . Jones and Alvy Moore. <br /> <br /> <br /> <br /> <br /> On a purely fun level . Hard to imagine Rochester to be athletes . Apparently Silver never wrote another screenplay after this was . But the great actor!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O2Iutmf6Crq",
        "colab_type": "code",
        "outputId": "31847bd3-1f7a-409d-a8f5-d5f75210bca8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "#sentences after pre - processing\n",
        "generate_sent(model, 200, random_seed=250)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'because this is a``Valentine Day\"event . The movie follows two young boys deemed kissing the girl, from the field and running for president . I will hunt them up at the clinic and then the first time up``a living from the boys find out who is fourteen and a scarf . A cool video to this complex story that begins the film\\'s wonderfully restrained here . <br /> This is not surprising . It\\'s hilarious death scene (if you try to be . But hey, it was funny as hell. <br /> Writer Armistead Maupin, and we see how Smith had an abusive boyfriend . Her relationship with Iris, clearly a serious drama into a semi-cheesy production, but they\\'re hot for each other on a literally filthy rich doctor (I was captivated by this trio of criminals . <br /> Fortunately, this is ONE of the cabins where pornographic material is funny as it always increases my appreciation for Rob Roy\", which take up arms for their lack of ability to make the Sea Monsters'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbNLdB5l6I6e",
        "colab_type": "code",
        "outputId": "da452ba8-771f-4d98-9059-cac04c007d84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "#sentences after pre - processing\n",
        "generate_sent(model, 200, random_seed=-10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'idea of re-uniting the members to justice, hope, but when you\\'re studying physics, and some of the movie, the woman whose parents were English . They colonized it, before revisions, didn\\'t speak English and Spanish on the silver screen . This film is enjoyable enough to realise just how real they kept me in for good . After knocking her up to the island commonwealth, and that detail are good as the first time in a movie well deserves the appelation . The first movie as it is a lesbian woman could fall apart due to a budget movie with his life and my friends on the rise of the things I\\'ve seen one of the sets are still widely available . Still these minor problems aside it was boring and didn\\'t think I\\'ve been dead!\"~One Of Andrews Most Memorable Lines In This Film is interesting even though ghosts are not that kind of laughter, adventure, emotion and sadness . When his fixation rewards him, she is hurt and kill people to tell him that we won\\'t be surprised that George'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr2iQHHb6L1X",
        "colab_type": "code",
        "outputId": "703f5270-f814-4de6-a0f9-e663cfac6534",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "#sentences after pre - processing\n",
        "generate_sent(model, 200, random_seed=33)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"hypnotic appeal . For some reason the United States got involved with the greatest trash ever made. <br /> Lastly, there were no problem; however the feel in the plot, but copied the format and incorporates it so easy (which proves to be a bit hit and bruised my leg badly just before moving her models into place; makes complete sense in a movie from an underlying sentiment of the most accurately written and profound . He is an official selection for the first time I cut his throat from being just another thickly formulated love story will play the lead.He was always one of my all-time favorite movie in digital format then I would recommend this wonderful movie, which is well worth it!! Great stuff! <br /> The use of the 60s (taking rare food neatly summarises their society: They're born innocent but is also the oldest ever British (I'm going to be both virile and gentle and kind of exorcism. <br /> <br /> <br /> Rating : 8/10 <br /> <\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7icezdd6OU4",
        "colab_type": "code",
        "outputId": "86fa2adf-969d-42d3-9a5c-e4af9bb0b82b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "#sentences after pre - processing\n",
        "generate_sent(model, 200, random_seed=69)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"near Lake Tahoe, just to see this film so loaded with human-looking Cylons, wound up in a cartoon . Most of the other side, especially remind us why this has aged remarkably well . Early appearances from Cherie Chung and Chow Yun Fat are also in terms of capturing the mood for a crazy-quilt hour of lame comedies that misfire, it cannot help but smile and a prostitute . Chances are that with his violent ways . A classic late 50's horror TV shows and movies, while together Keaton and Gedde Watanabe shine in their role well . My mom was complaining about CG animators). The acting is solid and suitable . This summarizes the other major and minor roles . O'Toole was long-established as a law reporter and then again what just happened. <br /> <br /> Miss Fritton adding to the early 1970's . This movie was the first season of Cosby is released . As a teenager, embarrassed by his boyfriend who brings the magic and mystery, something with great cinematography . The scenery (meaning it's a must see\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOylMnqZl88A",
        "colab_type": "text"
      },
      "source": [
        "# Calculating perplexity for statistical modelling positive reviews\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXqAwWdqmF_6",
        "colab_type": "code",
        "outputId": "15a48ee5-625a-4311-e6c1-587832224c70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "test_set = len(generated_text.split())\n",
        "\n",
        "import random\n",
        "r = random.sample(range(test_set),15)\n",
        "test_set_sample = [generated_text.split()[i] for i in r]\n",
        "\n",
        "import collections, nltk\n",
        "# we first tokenize the text corpus\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "#here you construct the language model for creating a dictionary\n",
        "def unigram(tokens): \n",
        "  # creating a dictionary   \n",
        "    model1 = collections.defaultdict(lambda: 0.01) #For words outside the scope of its knowledge, it assigns a low probability of 0.01\n",
        "    for f in tokens:\n",
        "        try:\n",
        "            model1[f] += 1\n",
        "        except KeyError:\n",
        "            model1 [f] = 1\n",
        "            continue\n",
        "    N = float(sum(model1.values()))\n",
        "    for word in model1:\n",
        "        model1[word] = model1[word]/N\n",
        "    return model1\n",
        "\n",
        "#computes perplexity of the trigram model on a testset  \n",
        "def perplexity(testset, model1):\n",
        "    testset= nltk.word_tokenize(testset)\n",
        "    #testset = testset.split()\n",
        "    perplexity = 1\n",
        "    N = 3 #change values of N for calculating perplexity of bi - gram or other models\n",
        "  #calculating inverse probability of occurence of words\n",
        "    for word in testset:\n",
        "        N += 1\n",
        "        perplexity = perplexity * (1/model1[word])\n",
        "    perplexity = pow(perplexity, 1/float(N)) \n",
        "    return perplexity\n",
        "\n",
        "model1 = unigram(tokens)\n",
        "\n",
        "perplexities = []\n",
        "\n",
        "for i in range(len(test_set_sample)):\n",
        "    pp= perplexity(test_set_sample[i], model1)\n",
        "    perplexities.append(pp)\n",
        "print(\"perplexity is\", np.mean(pp))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "perplexity is 2.9172088591914003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaFXyo-IAN22",
        "colab_type": "text"
      },
      "source": [
        "# Text generation for negative _reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBD_DLXlAm0Y",
        "colab_type": "code",
        "outputId": "bb5c4650-8d15-4b69-d7e6-b9039504c5af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "text = open(\"./negative_review.csv\", 'rb').read().decode(encoding='utf-8')\n",
        "extract = 0.1\n",
        "text = text[:int(extract*len(text))]\n",
        "\n",
        "#Mapping chars to integers\n",
        "chars = sorted(list(set(text)))\n",
        "# creating 2 dictionaries with character to integer and integer to character\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "#splitting sentences and creating an array with last character\n",
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "\n",
        "#reshaping the sentences into boolean so that it be passed into model\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n",
        "\n",
        "#creating a model\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars)),return_sequences = True ))\n",
        "model.add(LSTM(128, return_sequences = False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(len(chars)))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "model.compile(optimizer='RMSprop', loss='categorical_crossentropy')\n",
        "\n",
        "# helper function to sample an index from a probability array\n",
        "#I got this helper function from the lstm_text_generation example from\n",
        "#keras. https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "  # using a categorical distribution to predict the character returned by the model\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "# Callback function to print predicted text generated by our LSTM. \n",
        "#It prints generated text with 5 different temperatures [0.2, 0.5, 1.0, 1.2]. \n",
        "#0.2 will generate text with more ordinary word. 1.2 will generate wilder guesses.\n",
        "\n",
        "def on_epoch_end(epoch, logs):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "        print('----- diversity:', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        sentence = text[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "#for predicting next character\n",
        "        for i in range(400):\n",
        "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "#comparing loss after each epoch and saving weights with least loss\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "filepath = \"weights.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss',\n",
        "                             verbose=1, save_best_only=True,\n",
        "                             mode='min')\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
        "                              patience=1, min_lr=0.001)\n",
        "\n",
        "callbacks = [print_callback, checkpoint, reduce_lr]\n",
        "model.fit(x, y, batch_size=2048, epochs=10, callbacks=callbacks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_11 (LSTM)               (None, 40, 128)           138240    \n",
            "_________________________________________________________________\n",
            "lstm_12 (LSTM)               (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 141)               18189     \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 141)               0         \n",
            "=================================================================\n",
            "Total params: 288,013\n",
            "Trainable params: 288,013\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1095427/1095427 [==============================] - 80s 73us/step - loss: 2.7992\n",
            "\n",
            "----- Generating text after Epoch: 0\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"e A Nightmare On Elm Street Part Five a \"\n",
            "e A Nightmare On Elm Street Part Five a more the the the the sore the the the and the the the the the the the sore the the ant on and and the sore and and and the the the the the the the the the the wore the the the the the core sore at an the the the the the the the mont and and the sore the the the sore the soung the the the ming and the the the the the the the the the the the and the the the the the the the the the the the the the me\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"e A Nightmare On Elm Street Part Five a \"\n",
            "e A Nightmare On Elm Street Part Five a pile On the mestore the melant on thit hoad in toe in in ond hit fore thac mame anling thar this cone and ore fon ose toes an tore thee some a the tatt the s poreed the thin and the yort lathe corkat thor soke the tore and the worl mithe sor cave the ctoret titing the malt he ther the the male ther ant and fore for the poring bucnt the coens ant and and an the fure the wast and and some wot in the\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"e A Nightmare On Elm Street Part Five a \"\n",
            "e A Nightmare On Elm Street Part Five a cany atbaans en fom de niawm itid marl.ifoe autcauicle. Nresldamd andoDcarlort thergis apilly'siscis foo's toigky anes loy virkecandi\" be lpeg minl is lndsemnat, iegthir'fur indy povee d sakl tha monthinlad wamlemlaml taregbe ne an of worcewwon the, thelinpaL dis d ag san-lerlalaleelonthamcwumunt th at. Tot thes wiane asdeltang yy. DRinle-the mros digh wiuts there Tiaethe \"byollos.\"\", wasoud samer\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"e A Nightmare On Elm Street Part Five a \"\n",
            "e A Nightmare On Elm Street Part Five a soufwienzevis modctarp:ranct iphsra-y2's chicaso te movher asgunungeys Mogevieularalnet menghunliky tild thit ind amtimmedut, thasme1, taos an uvamd go s moave rase s Epcut Re)\".u\"ng trigishe P\n",
            "V<<3N/r8Lm races'srt ordias. /shre, tos Shomalatoo tre aretill tol en, C\"IA BroEklyow.<t, T OmMtorlis mungarkonccolle'cef rey beroe if vugalg. Hiwigo'cl oreheday.<rr, I9Louthevpicy saVZpwitnertfa 34- *os Th\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.79923, saving model to weights.hdf5\n",
            "Epoch 2/10\n",
            "1095427/1095427 [==============================] - 80s 73us/step - loss: 2.2175\n",
            "\n",
            "----- Generating text after Epoch: 1\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \" proceeds to try and kill this other guy\"\n",
            " proceeds to try and kill this other guy the was the fill the mane the mast the preally and the canded the film the and the manting the mase and the movie the wish the wast the whith the wast the and and the mand of the some the made the some and the soull of the and the misher and the mast and the probed the the wist the movie the and the wast and the mast the wast the and the fill and the and the wast the the wast the probed the fill \n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \" proceeds to try and kill this other guy\"\n",
            " proceeds to try and kill this other guy the soucher oun the in a deally for nether the ofpally and charting the mike the prearile he palpention in of the walt the neen wat be the mistallf on the and the bith the and the fild whe on his the proont and and the simely on the fill st somesten hor the fill the and the pater the mine for the bentent preal for whes wist at uther and enterlets thit it is to the gith how lanked and the same and\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \" proceeds to try and kill this other guy\"\n",
            " proceeds to try and kill this other guycom.<br /><br />Thid tus boentir.\"\" thale Afrilys alodes andinglt and and oche to kle cunwing of ths astroull semsoud dud in arstong storelygst. Engragiko Gapelleang in widstage the isef in baid be Rovie. Buchrloxe keregh fies tonis a cocolly that ab blaghess langer she Migeasided in uf on utoen. If be (he blited chobp caltigo ofboug thitgoutuch, whor. Sorese mailo utoy the coupe to Redtet the sum\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \" proceeds to try and kill this other guy\"\n",
            " proceeds to try and kill this other guying this?<Fr /><br />Bly be leaces od theraks, and beaves of the the of \"syevidkung on\"\"/0'ctermisedy<t, A>Ik, (I sting maou, be thig! Deos lat ghe semmecaryong puck .<br /><br />1O5,sinJ I's the Kadsy out\" strivins dorstool inotedserecice bunjor sloikurpartigs to greds, delmthy Wout't swasred or whag flind andaly at.<br /><br />I hasar simeruntit is cothrommong bbodter anuchane uos toererlicg of \n",
            "\n",
            "Epoch 00002: loss improved from 2.79923 to 2.21748, saving model to weights.hdf5\n",
            "Epoch 3/10\n",
            "1095427/1095427 [==============================] - 78s 72us/step - loss: 2.0063\n",
            "\n",
            "----- Generating text after Epoch: 2\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"mble in the Bronx\"\" that the west would \"\n",
            "mble in the Bronx\"\" that the west would the movie is a some to be the still of the store and the some the movie and the movie and the sters and the both the stor in the movie is a some of the stor the story of the was a could the while a prot of the story and and the the some and the film the was the stor and the bent the movie and the better and the movie that the store to be the and of the film the movie and the film the movie and the\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"mble in the Bronx\"\" that the west would \"\n",
            "mble in the Bronx\"\" that the west would wort a one this dis pare the scere that is was and and the rether the bikers on the is a fon this of the movie and in a cure and the stare this not of chare of the sture the stres and the story and the better for has intich, be the sere a to the movie in the rech in the was the rade and the acture the rene of the stors and the movie be the lot and the with where the movies at the stere has and the\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"mble in the Bronx\"\" that the west would \"\n",
            "mble in the Bronx\"\" that the west would bo suve remiles signtcsnavioricuce the purtodew. There are and thoughdy as somact-\"\".9) Iptertess in the bother fortame remanst ano hat to sen the if the hastinist on a gilg, out trey is the stix consopy picp (from bive abut'terer wats youtring of you hes it obe the revorts warl aT vusney the seral to ectleangrey on the his for do fim thaing huwing algiet has suese you reamintle; or kot sofle were\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"mble in the Bronx\"\" that the west would \"\n",
            "mble in the Bronx\"\" that the west would show bliteshorlved evoze evies. Not sthad wi'ls) to sich the fiem it hid, wonnd eader, drom the thong bute ag thoubsly (hofinale sume as viders nokred taget uT\",onefaralé 18S sory\"\",negative\n",
            "323,\"as brincved acto iver, bith that ilacous of the Bil''y all to mak-/ut the dock that distiny ant fer the (itingoned Sise't how wat wetes of not. I'n tthe withs aruch is dirna, the, the combive of tristions\n",
            "\n",
            "Epoch 00003: loss improved from 2.21748 to 2.00627, saving model to weights.hdf5\n",
            "Epoch 4/10\n",
            "1095427/1095427 [==============================] - 79s 72us/step - loss: 1.8794\n",
            "\n",
            "----- Generating text after Epoch: 3\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"e this movie nothing more than a low-bud\"\n",
            "e this movie nothing more than a low-bud the story and the movie and the film was the best the story have the story and and the best and the story and the story to be the story and the film is a some and the film and the story who have been the story the story was the story the story of the movie is the protion the story of the movie and the movie was the story this movie is the movie is a suppersed to see the story who the film is the \n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"e this movie nothing more than a low-bud\"\n",
            "e this movie nothing more than a low-bud the suring and the is not the how with the one to as a the film have wat and a film of the film has some to the 102. Somenore and the diss and the story in the can car on the bett stor in a who exsecting a comple and and the movie would and the spented to whit a make to be the shome is the with proplewer they have the movie of the film the simater come the film movie is a couple of the pilera bel\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"e this movie nothing more than a low-bud\"\n",
            "e this movie nothing more than a low-bud the is owh be movie spoodible the sommoning it with the sulpes for they were COw, and s olar af its movie, ever tough ans writh; don in sonothing into betiut our so dee thought suady she's and other that the how loouthliviemrackion the mokt rense his on the stowy scilwm.<br /><br />A, you non's with that I st scome his rimed?bby 19<br />Wikid't ubout things story's watcoes as dirably worse hel of\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"e this movie nothing more than a low-bud\"\n",
            "e this movie nothing more than a low-budne, - worke actwes this take, ru\"3ncaaschatializalSanisy\" and tor'c-?rictyad , in, Bundens -Da Julize with 2) Gilghe .!) asbey withet Jullary herrien is actual (\"\",nings, Cuppe-Agiy)\"\" UGbiest\",negavee312087 Thilas doindied a sext the fual mants whhinit. rimy in, In'ch this is list doely 'the movie! Friegct- hose and witle foor eve-kelor, in the 13. Lesn con shows framever arvaalto. Arteres <bra /\n",
            "\n",
            "Epoch 00004: loss improved from 2.00627 to 1.87944, saving model to weights.hdf5\n",
            "Epoch 5/10\n",
            "1095427/1095427 [==============================] - 80s 73us/step - loss: 1.7929\n",
            "\n",
            "----- Generating text after Epoch: 4\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"ness, however Boll somehow manages to fi\"\n",
            "ness, however Boll somehow manages to film is a was a some and the poor of the story was a complese and the movie was a some to see the film is not the story was a story of the seeps and the story and the story was the show and one of the movie was a something and the contered the film is a good to a comple in the film is a start of the movie was a still of the propering and the movie was a start of the worst and the worst of the movie \n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"ness, however Boll somehow manages to fi\"\n",
            "ness, however Boll somehow manages to film on the expected to every for the movie is mesters a movie. The stall of the reach the movered was to have the film with her in a movie seemed some of the movie was a diflected to be a problent and the show and want to so the film of the ship wish the silling. When the worderard the comperantion of the blood and seeps and the seepite is be a senting. The coment and the film is sack the movie of \n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"ness, however Boll somehow manages to fi\"\n",
            "ness, however Boll somehow manages to film maker you'm ig hands the quitfive,, homen that film this a child-foirard. the drom is natious as the ship ad betimpe. The gitt a film stming., berie tand is gut out promicilas and seaction, when shows his one wasn't ebbect making the verly wapes at Scene-and their I going thee dony Nadon was a go, bode's sureinely. The moot to bood at. The stire is sarresipinit of worst wance and when the  winc\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"ness, however Boll somehow manages to fi\"\n",
            "ness, however Boll somehow manages to fimm it heasains,.\"\" this great SDa tay eden, agconfstann wroding aboul thikg live with it.  AngrtE) hore. Wide's VUED filmdey Giar xuasons overoidaw, I wather cowms not triitling.2?.I-:0/997 onsmary.<br /><br />This mis act on Yon good..,negative\n",
            "2273,\"O diven eid.<br /><br />Onter ,negetave\n",
            "4715',\"I udsly dramings yoo jlmenth old tile out evollyswited paution. The you. The youh lavisuaro releaninc\n",
            "\n",
            "Epoch 00005: loss improved from 1.87944 to 1.79293, saving model to weights.hdf5\n",
            "Epoch 6/10\n",
            "1095427/1095427 [==============================] - 79s 72us/step - loss: 1.7288\n",
            "\n",
            "----- Generating text after Epoch: 5\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"pular brigade' that she ends up torching\"\n",
            "pular brigade' that she ends up torching the story was a secent of the movie is a some of the movie is a start and a start and the story of the one of the film is a seeps of the film is a film and the film that the story was a series and the movie and the film is a start of the movie and the film is a bean that the last of the film is a film and the movie as the film of the story of the movie is the film is a start and the story of the \n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"pular brigade' that she ends up torching\"\n",
            "pular brigade' that she ends up torching the bund she drean for the worst that the film casts to the acting to make the way in scene of the compinitely film of the starts of the stark fines, it could to think this movie was not a to of the mist of the film of cound that a interesting some and the end of the centing and been with the end of the secare with a performance comacter who the home as never in the something is a suppesions that\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"pular brigade' that she ends up torching\"\n",
            "pular brigade' that she ends up torching purt, the hound gons a unthing. when it sased of with.-The groat.<br /><br />in a because or bett?. Donly,..:the fints the sfory can action a0 millly can loddreas want with the onl, bussed as compinitemans out, follauble pommyotes food loins drell-dows who sunfinimal exuppla id. Ad motal direk over a simpiteral to stints, that is not reveels watnentitely no same of a<br /What, acting. AT LALLE dr\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"pular brigade' that she ends up torching\"\n",
            "pular brigade' that she ends up torching a bad :mbly most alox tame bight. Bit for alwaya. Oothand Rany Latsle Lack Wo sduenarsal. Le time. They ired wasnying the all working Cerrus crancings, 20 molies goes wrongs like timolious L*MTIFke's a was on a guren, diffor ole acting with cares, all. The end rewuidly. S as says that froung outris to really were and w sobe a child kougs, thistes a degcent has just upZass, it i bakt filfs (Arii: \n",
            "\n",
            "Epoch 00006: loss improved from 1.79293 to 1.72879, saving model to weights.hdf5\n",
            "Epoch 7/10\n",
            "1095427/1095427 [==============================] - 79s 72us/step - loss: 1.6792\n",
            "\n",
            "----- Generating text after Epoch: 6\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"ie wasn't anywhere near as 'Mighty' as I\"\n",
            "ie wasn't anywhere near as 'Mighty' as I was a movie and the screen and the film. The production and the acting that the movie is a better of the seen the movie is a problem that the story was the problem and the movie is a second that the probably and the movie is a series of the film was a some of the movie is a start of the only and the book of the story of the production and the story is the characters and the film is a movie and th\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"ie wasn't anywhere near as 'Mighty' as I\"\n",
            "ie wasn't anywhere near as 'Mighty' as I then the movie when the still of a did and of the how sidn't one of the only cares of a vidious in the lest the worst make for the could be berued and the worst had to have been to the post. The film was been to see the show and be many in the story his law story of the movie was a much still by the one commention of the director is an one and they are too scould have ender to her and something t\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"ie wasn't anywhere near as 'Mighty' as I\"\n",
            "ie wasn't anywhere near as 'Mighty' as I actean in to id awimens betwern, but surpling ovie what do uncamand pimalits to relately ghe careek to back hudious to for a ugul (horuabliess. Some is now her yeoser and he! Ameron, what wither the movie it. Seef for loubly beation into film Mana). I I'm toud mack hor tell agrors carts his for whil to ond in the 10 being a provired will alleks his, why atchyction, mane was takes a going nut to s\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"ie wasn't anywhere near as 'Mighty' as I\"\n",
            "ie wasn't anywhere near as 'Mighty' as I tood breillors as a saRmeake friends. Killy drectibul. is xoment he movie ham aufilments la amost fornen pirencars, did thits A consacted appeisry.<br /><br />Bit \"\"Sparts (it.\"\" Mad Thears is son't highly (think thruightory mostle.pese, the feether andy-, \"\"6 on such was theerh\") heurges and no eded drived, seeps no speyid unleed LRED Yard you, he Butten't have belled in can<by Othiclen: FoldaNt\n",
            "\n",
            "Epoch 00007: loss improved from 1.72879 to 1.67919, saving model to weights.hdf5\n",
            "Epoch 8/10\n",
            "1095427/1095427 [==============================] - 80s 73us/step - loss: 1.6401\n",
            "\n",
            "----- Generating text after Epoch: 7\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \", Ebenezer Scrooge is the grouchy cold-b\"\n",
            ", Ebenezer Scrooge is the grouchy cold-bad of the story is the story story is the worst movie is a start of the director was the characters the rest of the movie is a start of the movie is a story of the story is the story of the film is a better with a story of the worst contrised to be the could have been a sure of the time of the film is a good film is a start of the movie was a story of the movie is a complete that the movie is a st\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \", Ebenezer Scrooge is the grouchy cold-b\"\n",
            ", Ebenezer Scrooge is the grouchy cold-band and and the film was to all a creater who to the because the man actually is a senticulor that a movie was a surplite stack in a film better what even the only darge in the out of the out of the film would have been any the way and the show in the film is the take of the worst was the movie and was not essents in the only what is not story in the film but is a start slow the rest of a trues an\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \", Ebenezer Scrooge is the grouchy cold-b\"\n",
            ", Ebenezer Scrooge is the grouchy cold-bance annownd in this privises by after the carrots attrainationis movie so yus after new interest of pilt of character. The bade. Yu movie takes a be. U'wallow show see from my only to sterpores with at has a scenerity rititly atTrongle characters one actor mention cause work to hell to give \"\"\"be Ban suth actide\"\" Goden 30 it's short on u treatful ytory Shemoinancail is the father refurmance is t\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \", Ebenezer Scrooge is the grouchy cold-b\"\n",
            ", Ebenezer Scrooge is the grouchy cold-bestar, viny. This movie toumen! There's now white Fiquer KaB?..?P), by a wandesy of just goot nemay, oll ximfpent over, how! We'le sea fordoning arom stast usfety Crays/Reyme..The M\"! Biddey Hels film having precenbatimy almistabbey. Keatin tood-filmmon't-mevienfy or Rirectoh, Plose 30y, into this know dounble clemses poist of stwith MartPrifitis 6wa comeres about bed to down.<br /><br />Chararice\n",
            "\n",
            "Epoch 00008: loss improved from 1.67919 to 1.64014, saving model to weights.hdf5\n",
            "Epoch 9/10\n",
            "1095427/1095427 [==============================] - 79s 72us/step - loss: 1.6066\n",
            "\n",
            "----- Generating text after Epoch: 8\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \" into a mini-series or a sit-com, I thin\"\n",
            " into a mini-series or a sit-com, I think the movie is a worst of the story in the movie is a bean and the story is the movie is a story of the movie is a some of the story of the movie and the story is the film is a movie and the film was a starting of the movie is a story of the film is a worst of the film is a film of the story for the movie show in the story of the film is a story of the film what the film is a seck of the story of \n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \" into a mini-series or a sit-com, I thin\"\n",
            " into a mini-series or a sit-com, I think this movie is the real that which is the better show who is the movie and a movie in the story of a person story is a grands who all the scropped and romes to scene of the movie was one of the crap complete to promition and the good has the Jamol when I was because the really is anything in for the worst and a pretty of the compite of the movie seem a look and the first too imporsions that it se\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \" into a mini-series or a sit-com, I thin\"\n",
            " into a mini-series or a sit-com, I think!.. orly ducing person been fatiant, with helf sexmine that how povitry.<br /><br />9/B Brish Jaenabi Keaco-reslers, exuchable, slowding in the movie rude-boired inclotic compret of this intested bound a somoking plotion to fials of talist \"\"Bulde' lems what a centally as\"\". I cat unen, producal, even tear, I wolde fit was to are since to go so balk, a gets suppole of killer enjoyings that worse \n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \" into a mini-series or a sit-com, I thin\"\n",
            " into a mini-series or a sit-com, I think I muys not.<br /><br />This film doeps upits someonal.<br /><br />The 'anto re. Woolly many sibcederiting of the creeaindit of writh relestionely...\"\"<br /><br />? I is on' mening make to tmakey. Alta\"\"this dayin.\"So.-Mold plocsion and nothible thougslive as This Pry, Bod*\"\" Temba thlighopal (innomes clease, you warn't interss this dist. The ends (other meeting girl ploction \"\"The Eovie\"\", or wh\n",
            "\n",
            "Epoch 00009: loss improved from 1.64014 to 1.60657, saving model to weights.hdf5\n",
            "Epoch 10/10\n",
            "1095427/1095427 [==============================] - 79s 72us/step - loss: 1.5806\n",
            "\n",
            "----- Generating text after Epoch: 9\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"ocent unarmed man? The gun they had on d\"\n",
            "ocent unarmed man? The gun they had on don't see the director who is the story of the story in the story of the movie was a story of the complete for the story and the story is a story of the movie in the story of the movie is a like a film of the movie is a sen of the movie is the cast with a story of the movie and was a sense of the story in the story of the story of the story with the story of the story of the movie is a second of th\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"ocent unarmed man? The gun they had on d\"\n",
            "ocent unarmed man? The gun they had on deture a good with a girl movie the movie and some the sing. The complete problems for a little visable and meet that it sourded the meant as the complete to story and some can seem and wantion of the all show what were the story that he probably in the person was on the book and funny completely and seem it that the film is not a movie who see the film is a movie that would be the best of the movi\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"ocent unarmed man? The gun they had on d\"\n",
            "ocent unarmed man? The gun they had on doing.\",negative\n",
            "3607,\"she has every indended the direction and in a but beed tried in copiculath and the consished for a lack actor, and more to almo it is a stren sending cauns actors and suffeding cauping that they are holained and the filmally deally pint. The and the Bats and were gives it and a take him-as the character. Some. Oh did a girl kind of nover. I a mach really Viachappe ac and Pise\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"ocent unarmed man? The gun they had on d\"\n",
            "ocent unarmed man? The gun they had on don't stastupribted on sety!, Todie is doen a gype roat orbense. It's took charactey the quile she trablee Lanuoh is have it. I can't warte is most of & i no make that a tall film we were ty'm, do not liget, when I fan one as it aspist (nochized, me they are. Lot see it, sturry? No chentid' romagi, a witol retited hear tlotity,, and Mahby Bayin). Suther, fyow exeaping or a puehwarm soth Speets parg\n",
            "\n",
            "Epoch 00010: loss improved from 1.60657 to 1.58061, saving model to weights.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fccd4be8eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRqWIgj2Whgp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(length, diversity):\n",
        "    # Get random starting text\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    generated = ''\n",
        "    sentence = text[start_index: start_index + maxlen]\n",
        "    generated += sentence\n",
        "    for i in range(length):\n",
        "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.\n",
        "# predicting next character in the model\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "    return generated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOQEFbBUTvuu",
        "colab_type": "code",
        "outputId": "c1b1a597-12e3-44c3-d797-1d870c765aaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model1 = model\n",
        "generated_text = generate_text(100, 1)\n",
        "print(generate_text(100, 1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "crown things all up, the ghost who was offessed Plyse at the makes that they probably money on they can be an penfiling incenteor and she's \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad-ZjWG0w5Mk",
        "colab_type": "code",
        "outputId": "54ba0587-e225-41cb-fe1d-db6bff3a10f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(generate_text(100, 0.8))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ughter also adding in rude, disrespectfully have been engable real to be sumper aproming in the film. I can came to be surplite for a dree a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUubFhLaw9Yv",
        "colab_type": "code",
        "outputId": "f0691a6f-2f7b-4e7c-d8ff-859fa63119a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(generate_text(100, 0.6))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "s laid back \"\"cool\"\" style has ruined the bit that something and look of halls can madnical in a girl a deal real of the sige story in the c\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU7HE5_uxBEz",
        "colab_type": "code",
        "outputId": "a115d590-1eb8-4b04-9b92-bb2a5b25efb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(generate_text(100, 0.4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hirt off 3-4 times. Yes we all get it that the story waste the film in a movie of the book of a lot of the comerity of the story in the poss\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmq5xKmzxD-w",
        "colab_type": "code",
        "outputId": "fc38ae5f-6a31-4f8f-9c22-5098e296a27d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(generate_text(100, 0.2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " it's good\"\" deal. <br /><br />I wish I was the story of the movie is a film of a story of the story and the movie is a serious of the time \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_0KoUYqmzkn",
        "colab_type": "text"
      },
      "source": [
        "# Calculating perplexity for negative review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KiWivCkm3fs",
        "colab_type": "code",
        "outputId": "dde840ea-e58b-4eaf-9ccc-f7e6a166217b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "test_set = len(generated_text.split())\n",
        "\n",
        "import random\n",
        "r = random.sample(range(test_set),15)\n",
        "test_set_sample = [generated_text.split()[i] for i in r]\n",
        "\n",
        "import collections, nltk\n",
        "# we first tokenize the text corpus\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "#here you construct the language model for creating a dictionary\n",
        "def unigram(tokens): \n",
        "  # creating a dictionary   \n",
        "    model1 = collections.defaultdict(lambda: 0.01) # For words outside the scope of its knowledge, it assigns a low probability of 0.01\n",
        "    for f in tokens:\n",
        "        try:\n",
        "            model1[f] += 1\n",
        "        except KeyError:\n",
        "            model1 [f] = 1\n",
        "            continue\n",
        "    N = float(sum(model1.values()))\n",
        "    for word in model1:\n",
        "        model1[word] = model1[word]/N\n",
        "    return model1\n",
        "\n",
        "#computes perplexity of the bigram model on a testset  \n",
        "def perplexity(testset, model1):\n",
        "    testset= nltk.word_tokenize(testset)\n",
        "    perplexity = 1\n",
        "    N = 2 #change values of N for calculating perplexity of tri - gram or other models\n",
        "    for word in testset:\n",
        "        N += 1\n",
        "        #calculating inverse probability of occurence of words\n",
        "        perplexity = perplexity * (1/model1[word])\n",
        "    perplexity = pow(perplexity, 1/float(N)) \n",
        "    return perplexity\n",
        "\n",
        "model1 = unigram(tokens)\n",
        "\n",
        "perplexities = []\n",
        "\n",
        "for i in range(len(test_set_sample)):\n",
        "    pp= perplexity(test_set_sample[i], model1)\n",
        "    perplexities.append(pp)\n",
        "print(\"perplexity is\" , np.mean(pp))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "perplexity is 12.011017513076032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wyue3SB4IIJr",
        "colab_type": "text"
      },
      "source": [
        "# Text generation for Negative reviews using statistical modelliing(n-gram)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fwfEh7EZKOh",
        "colab_type": "code",
        "outputId": "51d8e442-570a-41be-808f-fb1873ffa2bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.lm import MLE\n",
        "from nltk import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "docs = pd.read_csv(\"./negative_review.csv\")\n",
        "extract = 0.2\n",
        "docs = docs[:int(extract*len(docs))]\n",
        "del docs['sentiment']\n",
        "\n",
        "texts = []\n",
        "for s in docs['review']:\n",
        "    texts.append(word_tokenize(s))\n",
        "#using nltk lm model for padding\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "train, vocab = padded_everygram_pipeline(3, texts)\n",
        "#creating MLE model\n",
        "model = MLE(3) \n",
        "model.fit(train, vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39WikD6SA5Q1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# processing formed sentences to remove unwanted characters\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "\n",
        "detokenize = TreebankWordDetokenizer().detokenize\n",
        "\n",
        "def generate_sent(model, num_words, random_seed=42):\n",
        "    content = []\n",
        "    for token in model.generate(num_words, random_seed=random_seed):\n",
        "        if token == '<s>':\n",
        "            continue\n",
        "        if token == '</s>':\n",
        "            break\n",
        "        content.append(token)\n",
        "    return detokenize(content)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tssYYDQnI9J-",
        "colab_type": "code",
        "outputId": "960a05f0-f461-4f84-afe2-6e317a0842d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "#sentence before pre - processing\n",
        "model1 = model\n",
        "word_list = model.generate(200, random_seed = 12)\n",
        "generated_text = (' '.join(word for word in word_list))\n",
        "print(generated_text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode of `` Blind Date '' ) is pure prime time television . If he really achieved after this site : it tries , and unfortuneatly , nothing worth talking about and does n't manage to realize early on that island in the genre . ( The television ratings system and my love of a crowd suffocating each other and I use the phrases correctly. < br / > Actually , this baby up at the Toronto Film Festival , I would love to both watch and arse-clenchingly boring . ( Even if there even * was a wonderful tour-de-force for Peter Sellers was one of the killer in the dig. < br / > The drugs part is the reduction of human co-existence . Alan , a great dancer and she kills the wan performance of Lucy Russell in the middle of nowhere and helps organize the first six minutes of the running scenes . Indeed , we kinda noticed she 's just say it again : `` This IS NOT ONE decent scare . The film 's inherent and unintentional humour . This is too `` complicated '' to get better or become unintentionally funny . Not that\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaH-tWksBAo8",
        "colab_type": "code",
        "outputId": "7baea07c-7d1e-4558-fc53-7e9abee515f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "#sentences after pre - processing\n",
        "model1 = model\n",
        "generated_text = generate_sent(model, 200, random_seed=12)\n",
        "print(generated_text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode of``Blind Date\") is pure prime time television . If he really achieved after this site: it tries, and unfortuneatly, nothing worth talking about and doesn't manage to realize early on that island in the genre . (The television ratings system and my love of a crowd suffocating each other and I use the phrases correctly. <br /> Actually, this baby up at the Toronto Film Festival, I would love to both watch and arse-clenchingly boring . (Even if there even * was a wonderful tour-de-force for Peter Sellers was one of the killer in the dig. <br /> The drugs part is the reduction of human co-existence . Alan, a great dancer and she kills the wan performance of Lucy Russell in the middle of nowhere and helps organize the first six minutes of the running scenes . Indeed, we kinda noticed she's just say it again:``This IS NOT ONE decent scare . The film's inherent and unintentional humour . This is too``complicated\"to get better or become unintentionally funny . Not that\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E23bzXD2BG3s",
        "colab_type": "code",
        "outputId": "8b1918e8-ae3f-4cdb-c12b-b95d58621b3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "#sentences after pre - processing\n",
        "generate_sent(model, 200, random_seed=250)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"bigger then the anchorwoman changed into a giant crash is heard in movie theaters . Children will want to look at necklaces from some of the film in mind, thoughts are churning:``What? The story makes it thrilling . Very funny: reminiscent of Disney . Only to have bottomed out here <br /> The premise was good! The book was the Holocaust. <br /> <br /> This is one of a film, it is at a friend who wants to be American and not go for these important issues, especially local ones . What was even better. <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> It's a total douche . He was only average (Van Bebble immediately stated that it would have appeared genuinely happy living in the film together, the hokey flesh-pigtailed flunky, that is?\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz9L-msRBamR",
        "colab_type": "code",
        "outputId": "cd67e456-1a24-4bea-aef4-77d1cb89985e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "#sentences after pre - processing\n",
        "generate_sent(model, 200, random_seed=-10)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'in kryptonite pajamas . That spoof worked very well . From there, I was one of the music for this on HBO2 . I wouldn\\'t allow dogs, which have no souls why not kill drug-sniffing dogs? Makes no sense (`` absent\") of Firecombe to hold out . The acting does not walk away and everyone else is just a mercenary? Hutton and Schlesinger don\\'t know (or can I picture hearing``motherf---er!\"on cable.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flNLWS-HBmFS",
        "colab_type": "code",
        "outputId": "899fe57a-8fc5-4bc4-c8c2-b1c442d0d257",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "#sentences after pre - processing\n",
        "generate_sent(model, 200, random_seed=33)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'in the same dumb mistakes literally hundreds of theaters across the stage in one word: VILE! I\\'m not sure that you will love this``Bangkok haunted\"are the real Jim Ellis, who\\'s lives . I an not adverse to a movie . About 30 minutes . If not, personally it didn\\'t know what happened, but instead presents himself at board meetings as a traffic jam for 20 seconds.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1bhvhpVBuxj",
        "colab_type": "code",
        "outputId": "a6556c53-b07c-4c8d-8210-cca065fe3d3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "#sentences after pre - processing\n",
        "generate_sent(model, 200, random_seed=69)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"not a big joke! What the hell is there anything that everyone else being killed around him, all is said in the opening scenes, but sadly becomes frustrating exactly because it's a few funny jokes, roll their eyes is one at a Southern Sheriff). <br /> <br /> <br /> I'm just saying that I remember them - it feels as though the town . The only real Lone Ranger, Charlie Chan, Fu Manchu offers a dose of wiz bang action and songs were created by the wisdom of its parts. <br /> Oh, the scene . Fourthly, the patient de resistance, little violence, dark humour plays a laughable Mexican bandit who can't really have to be really embarrassed that you have to have captured some aspects of this could ever synthesize with invented characters . Important dialogue was so stupid I found the dilettante Man Ray and Chicago were wonderful movies in and out cop who is wasted as Jane's Aunt no no no . America's greatest actresses of our Fathers and Bridge of\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELm-x01Ym-AA",
        "colab_type": "text"
      },
      "source": [
        "# Calculating perplexity for negative review statistical modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSCGfuCLnDUR",
        "colab_type": "code",
        "outputId": "b5c13c90-63b0-4f2b-cac0-79fc5f70e5c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "test_set = len(generated_text.split())\n",
        "\n",
        "import random\n",
        "r = random.sample(range(test_set),15)\n",
        "test_set_sample = [generated_text.split()[i] for i in r]\n",
        "\n",
        "import collections, nltk\n",
        "# we first tokenize the text corpus\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "#here you construct the language model for creating a dictionary\n",
        "def unigram(tokens): \n",
        "  # creating a dictionary   \n",
        "    model1 = collections.defaultdict(lambda: 0.01) #For words outside the scope of its knowledge, it assigns a low probability of 0.01\n",
        "    for f in tokens:\n",
        "        try:\n",
        "            model1[f] += 1\n",
        "        except KeyError:\n",
        "            model1 [f] = 1\n",
        "            continue\n",
        "    N = float(sum(model1.values()))\n",
        "    for word in model1:\n",
        "        model1[word] = model1[word]/N\n",
        "    return model1\n",
        "\n",
        "#computes perplexity of the trigram model on a testset  \n",
        "def perplexity(testset, model1):\n",
        "    testset= nltk.word_tokenize(testset)\n",
        "    #testset = testset.split()\n",
        "    perplexity = 1\n",
        "    N = 3 #change values of N for calculating perplexity of bi - gram or other models\n",
        "  #calculating inverse probability of occurence of words\n",
        "    for word in testset:\n",
        "        N += 1\n",
        "        perplexity = perplexity * (1/model1[word])\n",
        "    perplexity = pow(perplexity, 1/float(N)) \n",
        "    return perplexity\n",
        "\n",
        "model1 = unigram(tokens)\n",
        "\n",
        "perplexities = []\n",
        "\n",
        "for i in range(len(test_set_sample)):\n",
        "    pp= perplexity(test_set_sample[i], model1)\n",
        "    perplexities.append(pp)\n",
        "print(\"perplexity is\", np.mean(pp))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "perplexity is 2.1926037074372404\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMS1As4vR1QO",
        "colab_type": "text"
      },
      "source": [
        "# Text generation for all reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o3DlRQqR5_C",
        "colab_type": "code",
        "outputId": "fbba937f-6909-4d43-801e-2aa304469e68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "text = open(\"/content/drive/My Drive/Colab Notebooks/IMDB Dataset.csv\", 'rb').read().decode(encoding='utf-8')\n",
        "extract = 0.05\n",
        "text = text[:int(extract*len(text))]\n",
        "\n",
        "#Mapping chars to integers\n",
        "chars = sorted(list(set(text)))\n",
        "# creating 2 dictionaries with character to integer and integer to character\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "#splitting sentences and creating an array with last character\n",
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "\n",
        "#reshaping the sentences into boolean so that it be passed into model\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n",
        "\n",
        "#creating a model\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars)),return_sequences = True ))\n",
        "model.add(LSTM(128, return_sequences = False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(len(chars)))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "model.compile(optimizer='RMSprop', loss='categorical_crossentropy')\n",
        "\n",
        "# helper function to sample an index from a probability array\n",
        "#I got this helper function from the lstm_text_generation example from\n",
        "#keras. https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "  # using a categorical distribution to predict the character returned by the model\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "# Callback function to print predicted text generated by our LSTM. \n",
        "#It prints generated text with 5 different temperatures [0.2, 0.5, 1.0, 1.2]. \n",
        "#0.2 will generate text with more ordinary word. 1.2 will generate wilder guesses.\n",
        "\n",
        "def on_epoch_end(epoch, logs):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "        print('----- diversity:', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        sentence = text[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "#for predicting next character\n",
        "        for i in range(400):\n",
        "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "#comparing loss after each epoch and saving weights with least loss\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "filepath = \"weights.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss',\n",
        "                             verbose=1, save_best_only=True,\n",
        "                             mode='min')\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
        "                              patience=1, min_lr=0.001)\n",
        "\n",
        "callbacks = [print_callback, checkpoint, reduce_lr]\n",
        "model.fit(x, y, batch_size=2048, epochs=10, callbacks=callbacks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_13 (LSTM)               (None, 40, 128)           137728    \n",
            "_________________________________________________________________\n",
            "lstm_14 (LSTM)               (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 140)               18060     \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 140)               0         \n",
            "=================================================================\n",
            "Total params: 287,372\n",
            "Trainable params: 287,372\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1103326/1103326 [==============================] - 79s 72us/step - loss: 2.7730\n",
            "\n",
            "----- Generating text after Epoch: 0\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \" I have liked her in other movies. The c\"\n",
            " I have liked her in other movies. The core and and and and the the the the the the the the the the the the the soris an the the the the and the sore the the the the and the the the the the the the the sant and the sore and and the the the the the the mist and whe the the meris the the the the the more and and the sore the the the the the the the the the the sithe and the the and the the the the the ther the the the the the the the sere\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \" I have liked her in other movies. The c\"\n",
            " I have liked her in other movies. The ceresanle The e veor the wore fart and and or the lis ut the garle the the it erar alle sore the cilis the molis ot the mode in thur the sorelen was il a fere san ghe memuve tor thas aret sher I the sor the ind on and atte the lrand brer. The soll in the the tor jist tit the worle. An ind bon the junt the fars the sand bud bas the thit and the the perre tore the mother it'cs the milos on or tere ho\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \" I have liked her in other movies. The c\"\n",
            " I have liked her in other movies. The ckalle hoca ghe gust meld \",ould ongore Dinalin haripkigit and therely oxs. Lor claok prerlind Wocpelean, duw thit\"rfobec, thy wat onoil soe tse as ardtorg anddatafl oan woslet rubilg\n",
            "sol eve won fh thers gorey afureten Bovibs he I'lrlf kadit on ame aferri. It manthis. /pe..<br />Het teent. Alf ount thover ilvene, loce watise anperysasamtezilungishe ir piauske thet pread of epdibe candeliny shed ci\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \" I have liked her in other movies. The c\"\n",
            " I have liked her in other movies. The chaggawe andiane ths Guosenh Sowirk, flry. Is ijlelsddyrent,n; arlederf tharere mictary heayiad invas'edy Kaagiyjlk.:bny as hays*ly!\"sd covemqiurb Mrusoy t trirs winnlenry..Db>arldufhaw haa, rriby mEwheu Oolkete\"nincor,-OWo kovuy I. wat riskl, metama perineltite\"\",. wuT. sha ip Gobkopuvy acist at0 anmereavea Fatfive tlor Jevo jrans ce li S seblas toe hI nrik Banlys atto aens to mrPoihs rox verathy \n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.77297, saving model to weights.hdf5\n",
            "Epoch 2/10\n",
            "1103326/1103326 [==============================] - 79s 72us/step - loss: 2.2143\n",
            "\n",
            "----- Generating text after Epoch: 1\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \" and the humor makes no sense. Whether y\"\n",
            " and the humor makes no sense. Whether you mand the soud the sere the move the some and the sout of the store the store the sere the sher store the store the store the and the sere the stord and the store the sere the sore the store the mevie the sere the and the mand the mave the some the sound and the pert the sere the sould and the sound the shore the movie and a dand the sere the sere the sear the sere the and store the sound the so\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \" and the humor makes no sense. Whether y\"\n",
            " and the humor makes no sense. Whether you sto panding the and the harl of mis sous frer mome the and the made sound soud and the perent the sope with the veres of the and is to the movie reper for the meving and and a to that the sounting the houk and comes and the pland love cond the daad the rist the bener har sound the ald rot and fer a cont of whith come han bed rast the a prond the for san this far and sher thing and ard and hove \n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \" and the humor makes no sense. Whether y\"\n",
            " and the humor makes no sense. Whether yeve is Chigh in ip the inen firts be dein jursitad T\"<rr /><br />WAnng it'viccay, buos is Avore, nas ald grom theck the ceviats of and the wloard, in it canly wouk is chags I, bespes. That ome soud'l bey of matting I wisd by in not on the ard bever.<br /><br //To ronin't cand frof ofuseles fet it o-a Thilyuble I Mefliadly wall \"\"1n obe s'jna'st.\",'vingatlr Camt of the some bes mat to hers store su\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \" and the humor makes no sense. Whether y\"\n",
            " and the humor makes no sense. Whether yceiok the Ton's SnLikn Bildin it cameles bangt guore. Acprloy, thepetert on tis in is Fanlile Jouke sus aimvice shave A,t. Tive soy casted an' ver vulik. it one hompe in amlast mithed quericce of what talk culd nochers, is pust. Tkind aloth car, y erpiplly Asraits Tes of goed inud Piyre. (her mpmoternw/Nacle Cunt in Ge:bar.?), sny turn'l 're fix of trronpr,, pope onsyfyruled withs fas mos one thi\n",
            "\n",
            "Epoch 00002: loss improved from 2.77297 to 2.21432, saving model to weights.hdf5\n",
            "Epoch 3/10\n",
            "1103326/1103326 [==============================] - 79s 72us/step - loss: 2.0044\n",
            "\n",
            "----- Generating text after Epoch: 2\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"fe and death\"\" than Capote could ever cr\"\n",
            "fe and death\"\" than Capote could ever cranter and the was a like the movie that the some that the movie and the movie and the hard that it is a secters that is a show the movie is the film and the story and the movie and the part and the some of the shat that movie and the seally wather the show the movie and the movie the story and the scraes the shat the movie and the sere the movie of the story and a like that the movie and the some \n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"fe and death\"\" than Capote could ever cr\"\n",
            "fe and death\"\" than Capote could ever cremates what is wat and wat to being a really and have move movie of the may of aster becomes of the dill film of the reating a conter the beshar as of the movie of a most of what the senters movie and the and semance it a vinger in the sact call as of a domar in and have the evie and the watter was worl as sume that is a way the movie is storbent of a ferting to the was chare thar door and the bou\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"fe and death\"\" than Capote could ever cr\"\n",
            "fe and death\"\" than Capote could ever cralsial that fulm. It Woply wall a beremh is a poindlities betorent thar tharn to grom. Thay ond ance is theing a boye hawter, Itending say, stilmegorameater, and that im a. It'ce thinds mution wiskcowigaritact acmont of she watecher bloakces! Thas goes is and coppmotiad wews muttocs semoviovertay firts esoth wick and ofd is it tumie pay's a usty on you sthick mact of the bitsner ge-ulffregic: ade \n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"fe and death\"\" than Capote could ever cr\"\n",
            "fe and death\"\" than Capote could ever crast dunl in'h gincsobef misthid of Thi\"\"ssatap's.\"<or />An ReGN3.s of the gott boperapesa-nopedunn memer.<rr A><br />quipes u coms7bue theing omt riglos played in in who gote's contarivemcthet. As simnegend nuclers ylomadtios rayever, dowrute fut fe.?.. Som'bucial in that thistest in SHmwccinacicrara rowsceriout inbee a thit havands, pustibut onharlales-sume sfucs, gadtanbe?<br /><br />And yoks ou\n",
            "\n",
            "Epoch 00003: loss improved from 2.21432 to 2.00445, saving model to weights.hdf5\n",
            "Epoch 4/10\n",
            "1103326/1103326 [==============================] - 79s 71us/step - loss: 1.8773\n",
            "\n",
            "----- Generating text after Epoch: 3\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"y successful films, he has taken many ch\"\n",
            "y successful films, he has taken many character and the scare to a some a story and the story was a such is a seep to the see some to the film of the story and the story was the some to the see in the see see the story for the see to the see some in the beally seen the see the see that the see the see to the seen a story and the see the see the see that the film the and the story and the some a beally see to the see the film and the pla\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"y successful films, he has taken many ch\"\n",
            "y successful films, he has taken many characters in the performantion. The sade see of the ton when the cart in the diresterss and that the ouse ground before and a sere to the film and for I an and interesting to be a some intere of the stull was a film some to the vied how it is a film work a movie as unters and sere inthat watch it a proded and scene the movie worls, the sould of the send to be film. But the story to film and the bes\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"y successful films, he has taken many ch\"\n",
            "y successful films, he has taken many charets, hele nood, frelive intesting caonictly pialoms abcouttre toles snoigh ampit movie mare by linal ary she wilch hat a bood a primy aschistent. The cennes with a profers tnoty represo or of ?\" sceints on cramated Bulgran Perd's eming aspiat ackually yous mind was but inthand viesd.\",positive\n",
            "\"Jeidn'm starty un cindom not to oll movie is of yoo daliving of yumed, to clanh whhat'he's epensed rec\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"y successful films, he has taken many ch\"\n",
            "y successful films, he has taken many chly desting wipt weat drid't ifieles deegra-decliffiod ploblring stase, suywes sgudune ougdnting nesners,?<br /><br />This as verree somes is fiible.<br /><br />metter, eallys, verar and that has see over jullseng wortching 29georass.. vereing at protade! Qrienneds'.. (he its ampil crustnal to seop vireing are ploss, whine, yon wasn that cold, erdeart fallies around disoptiinht book oftens. Co time\n",
            "\n",
            "Epoch 00004: loss improved from 2.00445 to 1.87732, saving model to weights.hdf5\n",
            "Epoch 5/10\n",
            "1103326/1103326 [==============================] - 79s 71us/step - loss: 1.7895\n",
            "\n",
            "----- Generating text after Epoch: 4\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"ot for everyone, but it divides its stor\"\n",
            "ot for everyone, but it divides its story and the movie was a such and the story of the scenes and the seems so see the see in the seems and see the see a sere that is expected and the movie was the see the movie is a see that well a see the see some with the film and the see a film is a complete to see a see the part and the movie and the movie is a see the see a was a see the see a was a see a way a see stare to the see a complete of \n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"ot for everyone, but it divides its stor\"\n",
            "ot for everyone, but it divides its story and a sere a foring in the story and it as a completion perform to the show of the bust and a crist of a some a more the deventional some is a complete in the film seem a film was part to be the real showe in a near looked and the like the film is don't been the show that a slave stentions and stark is film make we have a way, a deason give for the movie is a crade to be and the end teal film th\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"ot for everyone, but it divides its stor\"\n",
            "ot for everyone, but it divides its storaghe and sceance turture, the fill is to was hickred diy beand unjent of the amy Geard.<br /><br />Well innally with bihch teoned poone eshels!, and ale ould wonder mighting. can expefted of the erear, in the makispes, with the ors to their oney. It to profor with olky, the bryand is blump tumor' walt, but as now bestorisalaly too Owhich brauce untaves, was cherestan. While getter that Neal have b\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"ot for everyone, but it divides its stor\"\n",
            "ot for everyone, but it divides its story) of Aler Hurroor, fwact, and How for seen to effices in the noxs.\",nigative\n",
            "\"I amla comie:. Won, misst on the moveorix cimeda one axmeffowite realing the har-entess onlods.\",negative\n",
            "\"A decistadeofore mase movie as. crasels out begorint tide the probelings,), you 'mant, we. I saw here  dufb in the about ouch-use oft's 'lef Art Peat, vereod wo bavetes werap, buthere out by movie to hel by telling\n",
            "\n",
            "Epoch 00005: loss improved from 1.87732 to 1.78952, saving model to weights.hdf5\n",
            "Epoch 6/10\n",
            "1103326/1103326 [==============================] - 79s 72us/step - loss: 1.7255\n",
            "\n",
            "----- Generating text after Epoch: 5\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"s as reasonably intelligent human beings\"\n",
            "s as reasonably intelligent human beings and in the movie was a see the complete of the film was the see a see a story the film was a story of the movie and the movie story and the movie was the story of the acting that the play the story was the port of the characters with the film who is a some of the story and the play and the movie was the story and the movie was the film is a film is a series and the film and the best the the the t\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"s as reasonably intelligent human beings\"\n",
            "s as reasonably intelligent human beings and trues of the movie and the shought to it a courd for this poor all the which only goes real that the film say is sturder of the action of the was a film was down in the man film. The movie of the long of the house and the actors and there of the film of the hisse of the excopt to the had to be actors and such a film and the plot have film performances, but the tare to that the movie like a fi\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"s as reasonably intelligent human beings\"\n",
            "s as reasonably intelligent human beings in seciliting! Rest in or, stick of the must a fusey evanyul pecpon, centing Storraved is the actrent to a the centurionartur and gorise thremidications cleships, apporsing the stase alsightoned, for sajer, but Inames.,notatixal tom\"\" The flrattre simblets, Anckinkenes. 1 Marriedle, it was (Sunne Vatchark's plared. Do. Wo gaind whan that them arveacted rolile scened. They was more of the life Jal\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"s as reasonably intelligent human beings\"\n",
            "s as reasonably intelligent human beings, the 70 - mo once an eldoced.\",pusicial qusie ot texcally treation were usel Roy Barked!\" (. I (all very stybrouter, a siny movies most - brind wouse knoigluse verring rigurious... Loke doestail entigist-fact gredy to story lace pottulicap life raliectic towarrs to ever fun To Daphibet watched it's luly have gase rodief thrung, tear fraturally7? He is werew has Pearasses in besalious, veribed cor\n",
            "\n",
            "Epoch 00006: loss improved from 1.78952 to 1.72547, saving model to weights.hdf5\n",
            "Epoch 7/10\n",
            "1103326/1103326 [==============================] - 79s 71us/step - loss: 1.6759\n",
            "\n",
            "----- Generating text after Epoch: 6\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"le dialogue, this mind-numbingly awful d\"\n",
            "le dialogue, this mind-numbingly awful destring to the sumple the movie is a film to the film that is a supposed to the story of the story is a movie in the film of the story of the film is a start that the film the movie was the story is a surtral thing the story of the film that is a star who have to be the see and the story of the movie is a street and the story of the film is a show and the series and the story with the film that th\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"le dialogue, this mind-numbingly awful d\"\n",
            "le dialogue, this mind-numbingly awful did be a fairly and interesting the movie and the movie is a story distors as a unto this for the film is part and seem to the seemes a but it is a complete to go down that most work the movie with the story and supposed the sater the comedy and this man is a complete to really on the direction who becially but the like it was just him of the film and watch the rest the cript can to the secies of t\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"le dialogue, this mind-numbingly awful d\"\n",
            "le dialogue, this mind-numbingly awful due was sume kouch three hour I is the thors are showbings maken thy hahvers bad much up Linon Wich mitlok of the trible, who bean thing, the cheems now day goony ut.<br /><br />Themal film give the waster. Othing it again peff. He oven lems too very one as a did performance of stips and it is word not renope fro-tonanodly bush lungs, showfy dsenly is this (hure take but have a some. Amen's scare b\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"le dialogue, this mind-numbingly awful d\"\n",
            "le dialogue, this mind-numbingly awful daven.\",)-han injealoshy, it's beroused Duan-'s. A his-movinittant with one cass. My.),positive\n",
            "\"Ropothingian wrathef brourgors of abso brouchly (protty star now's to cVmidadide hassamurais phisisy coint say whereed anytwatching ) I's not marret. Indecting you ories acsinal wandsiinds, of MV cromalital muser's sup frommentady entocts, masory inout and brenk.<br /><br />There all exarted in here are\n",
            "\n",
            "Epoch 00007: loss improved from 1.72547 to 1.67590, saving model to weights.hdf5\n",
            "Epoch 8/10\n",
            "1103326/1103326 [==============================] - 79s 71us/step - loss: 1.6381\n",
            "\n",
            "----- Generating text after Epoch: 7\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \" this is not one of those classic tear-j\"\n",
            " this is not one of those classic tear-jom and the movie to the movie and the movie that is a movie and the movie that a great that the movie is a character and string that the movie that the movie is a story of the film and the movie was the story of the the the the can of the movie is a can in the movie is a contrain the movie and the plot of the characters and the film is a film and the movie was a start of the characters and the man\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \" this is not one of those classic tear-j\"\n",
            " this is not one of those classic tear-just and really seen that the movie was all the movie was the film that the way of the the movie is a mind the movie for the general fall and proveting of the characters and the hamore fan better to contry of a had many that the show to her movie she story of the movie was a the fact and inspecting and has the ere can in this movie character in a great is a consuptic character of the sex as the fil\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \" this is not one of those classic tear-j\"\n",
            " this is not one of those classic tear-just murdes that can tell really remoments the stenion blacks unto much time and the goribal the cent.\"\",negative\n",
            "\"Meombret stants! Pet movies factions at and there is the great pince. I an 1900d. Joh' Geou, O OUM.*00((Fagen higrlyturs. Athough the acting for just pombor throush, do he. Memage the aren only last he. Nect at sat had of ampoteximing and ve ofter Fine of Frang Preik Courge's deaming d\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \" this is not one of those classic tear-j\"\n",
            " this is not one of those classic tear-job on grear-lot-futly. GRhyod DELES spealaying, *9Ty Reviluis, a mea some ismone quarual heartzing Nuhs one for he sarcuals tegereraly strenglotic as.<br /><br />This movie. Alming. Dmikeinss whichs with Oman asSelfts,)...yever help Cistule, Hando, A-Dudk.<br /><br />Tannabsnatoly bayples,;.\",positive\n",
            "\"I a kid screen, not enoughted to she me Monew-wilm of good)),ubbit is upsy for caned Broyin in M\n",
            "\n",
            "Epoch 00008: loss improved from 1.67590 to 1.63811, saving model to weights.hdf5\n",
            "Epoch 9/10\n",
            "1103326/1103326 [==============================] - 79s 72us/step - loss: 1.6056\n",
            "\n",
            "----- Generating text after Epoch: 8\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"e wonder if everyone will ever be able t\"\n",
            "e wonder if everyone will ever be able to see the seems to the movie with the story and the story of the story in the tries to the than the movie is a good and the story of the film was a sure the realing and the story to see the story to the story of the movie is the sure the movie is a surprise to the story of the movie in the movie is a complete to the film is the story to be a some and the story of the constant to the story of the b\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"e wonder if everyone will ever be able t\"\n",
            "e wonder if everyone will ever be able to this such to some of this movie that the could really in the one to life which is not be the can to have this movie was so many point and the commanitatiss are the gard to the life to the film shows the boot of the film of the transand in the film does the tranger in the film make a some in the movie in a not of that come of a comedian was every prant creating the realing and the same in excelle\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"e wonder if everyone will ever be able t\"\n",
            "e wonder if everyone will ever be able to a comples-book for the acnies Frem Bands who which one wasted to  write Corerga, this, relibiously pargy need that moneyer daally readink on excertly end ton Freviesips seqoine, I-makes like the with. Nod terry drives refureting or totoll after the game of Dingtime time.<br /><br />Yow dif ip well. Casteresterly which themerware dariente lere that mustive in this feel charest and performance. Mr\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"e wonder if everyone will ever be able t\"\n",
            "e wonder if everyone will ever be able to commastery umention of lilable. 8, LBEGDICman, enker who wish peracredifuly memorabial Dattil ob ofther, his one's releire ut-illy reverient she day gony, and made deach hoirs movie's excasedorion: and of his work.<br /><br />Shanly aday Prum your. I beed bad leadn't had to see, so money the klove wro atallihe hadge somit. That framarishal Jikereru Dappodel .<br /><br />I absoll go a. Bory! Ever\n",
            "\n",
            "Epoch 00009: loss improved from 1.63811 to 1.60558, saving model to weights.hdf5\n",
            "Epoch 10/10\n",
            "1103326/1103326 [==============================] - 79s 71us/step - loss: 1.5794\n",
            "\n",
            "----- Generating text after Epoch: 9\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"e Geraldine McEwan version suffered from\"\n",
            "e Geraldine McEwan version suffered from the show the story of the acting is a complete of the movie is a real of the movie when the best and the story of the movie that the movie is a good that the story and the story and seemed the story that the story the the the the problem and the story and the story and the story the story and it is a real that the movie that the acting the story and some of the worst and some of the story and the\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"e Geraldine McEwan version suffered from\"\n",
            "e Geraldine McEwan version suffered from the really seems to kill the story for the great other to give this movie actors. The story is her a movie that the more to be the film is her thing the mals from the strungle fact of the way this film is a good for really away the best makes a real with film. The film great this movie films and the broles and acting the acting back and the suits and fall to simply and a great film in the plot of\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"e Geraldine McEwan version suffered from\"\n",
            "e Geraldine McEwan version suffered from s mity where it was everythance usen Cort of Tatti. THE STCTLO9LDAN<br /><br />1.\".pa with upon arountic peasing-tof some killing the expiny atching?. Then killy, the fitten Sicto Herlies, Dan Waltten has in a twidn for the bore, me walt to pary on abperiant will all unreilly sope-all the Reemstisie's Abemen and Anrish is for good of 12 looking and including the probably video crumb of 1989 10 hi\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"e Geraldine McEwan version suffered from\"\n",
            "e Geraldine McEwan version suffered from the pictor gave for a slon for not anywadey and I on a character because it far to pregisity prytadine crude in ha proglay chointly she isline prefection Wustraturia. Samic and a way thlownat Che Git PabEir's in the Custy. Pyyney)', Jove was nide fimm -ulidedather brolufaturds time brows, enjestent near lookel J. Momp -form definedrent a telliborm Knolling (as get a charage it:wal and some itself\n",
            "\n",
            "Epoch 00010: loss improved from 1.60558 to 1.57944, saving model to weights.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fccd5367fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nunwpuV0Wuyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(length, diversity):\n",
        "    # Get random starting text\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    generated = ''\n",
        "    sentence = text[start_index: start_index + maxlen]\n",
        "    generated += sentence\n",
        "    for i in range(length):\n",
        "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.\n",
        "# predicting next character in the model\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "    return generated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ji8Z6K-oTxPj",
        "colab_type": "code",
        "outputId": "eb6b1e5c-8250-49f2-f6de-52ecb63f9493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model1 = model\n",
        "generated_text = generate_text(100, 1)\n",
        "print(generate_text(100, 1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "funky way. Apparently people either like An Krant Rademon due making scienita is simple. Some all them actures deeriot and while mess stup o\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYk8KnrVGOdz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('/content/drive/My Drive/Deep_Learning_models/NLP_all.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vib2jVuU0lCu",
        "colab_type": "code",
        "outputId": "3956cdef-1c28-4ddb-ebe4-e63ba3cea749",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(generate_text(100, 0.8))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "only one. Taylor reportedly hated it and we looks Abirot' But work suppen tear or the camority and a martion movie though really nidely. The\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhQkS00n0oCU",
        "colab_type": "code",
        "outputId": "1b40a8b3-e036-4c41-f0bf-aa66574f53e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(generate_text(100, 0.6))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ut of Evil Dead II). There are, however, was a director movie show directing the show in the master in this better way the screen and someth\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5fuxEt_0qX0",
        "colab_type": "code",
        "outputId": "0c7da69c-a082-492a-bf14-b895666b8a31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(generate_text(100, 0.4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "s, ever. Just beautiful, full of human ender the fact the are about the film and it than the movie is a sure of the have the main and the mo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3roUVgXY0uI0",
        "colab_type": "code",
        "outputId": "82168ebc-24d4-4787-ca81-bb6992de9e72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(generate_text(100, 0.2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "o be good because I'm interested in the screen and the movie is a movie and the the the the the the the the the the story of the movie and t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9U_-SYKne2X",
        "colab_type": "text"
      },
      "source": [
        "# Calculating perplexity for all reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UPDomGLnjPo",
        "colab_type": "code",
        "outputId": "83308307-380a-4e18-9776-4c55274e34f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "test_set = len(generated_text.split())\n",
        "\n",
        "import random\n",
        "r = random.sample(range(test_set),15)\n",
        "test_set_sample = [generated_text.split()[i] for i in r]\n",
        "\n",
        "import collections, nltk\n",
        "# we first tokenize the text corpus\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "#here you construct the language model for creating a dictionary\n",
        "def unigram(tokens): \n",
        "  # creating a dictionary   \n",
        "    model1 = collections.defaultdict(lambda: 0.01) #For words outside the scope of its knowledge, it assigns a low probability of 0.01\n",
        "    for f in tokens:\n",
        "        try:\n",
        "            model1[f] += 1\n",
        "        except KeyError:\n",
        "            model1 [f] = 1\n",
        "            continue\n",
        "    N = float(sum(model1.values()))\n",
        "    for word in model1:\n",
        "        model1[word] = model1[word]/N\n",
        "    return model1\n",
        "\n",
        "#computes perplexity of the bigram model on a testset  \n",
        "def perplexity(testset, model1):\n",
        "    testset= nltk.word_tokenize(testset)\n",
        "    perplexity = 1\n",
        "    N = 2 #change values of N for calculating perplexity of tri - gram or other models\n",
        "    for word in testset:\n",
        "        N += 1\n",
        "        #calculating inverse probability of occurence of words\n",
        "        perplexity = perplexity * (1/model1[word])\n",
        "    perplexity = pow(perplexity, 1/float(N)) \n",
        "    return perplexity\n",
        "\n",
        "model1 = unigram(tokens)\n",
        "\n",
        "perplexities = []\n",
        "\n",
        "for i in range(len(test_set_sample)):\n",
        "    pp= perplexity(test_set_sample[i], model1)\n",
        "    perplexities.append(pp)\n",
        "print(\"perplexity is\" , np.mean(pp))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "perplexity is 6.986476760727675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uErSv0-WJEnO",
        "colab_type": "text"
      },
      "source": [
        "# Text generation for ALL reviews using statistical modelliing(n-gram)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEsGzy2gJLVO",
        "colab_type": "code",
        "outputId": "6fcc54a1-b334-44e0-b38a-a238185655cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.lm import MLE\n",
        "from nltk import word_tokenize\n",
        "# we need to download a special component that is used by the tokenizer below -- don't worry about it. \n",
        "nltk.download('punkt')\n",
        "\n",
        "docs = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/IMDB Dataset.csv\")\n",
        "extract = 0.05\n",
        "docs = docs[:int(extract*len(docs))]\n",
        "del docs['sentiment']\n",
        "\n",
        "texts = []\n",
        "for s in docs['review']:\n",
        "    texts.append(word_tokenize(s))\n",
        "#Using nltk lm model for padding\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "train, vocab = padded_everygram_pipeline(3, texts)\n",
        "#Creating model\n",
        "model = MLE(3) \n",
        "model.fit(train, vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9QjGKeuJpfg",
        "colab_type": "code",
        "outputId": "dc934eea-c55e-43d1-cb90-cde18935ad47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "#pre - processing is not done for this model in order to show difference between outputs\n",
        "model1 = model\n",
        "word_list = model.generate(200, random_seed = 12)\n",
        "generated_text = (' '.join(word for word in word_list))\n",
        "print(generated_text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encouraging his pack 's hands ; however , the music is just a little black girl shouting crude insults to him . This Lifetime-like movie was made , well worth it . </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h333EqxV1MjA",
        "colab_type": "code",
        "outputId": "55101ea6-316d-4ff8-dad3-238df3c93b3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "word_list = model.generate(200, random_seed = 250)\n",
        "print(' '.join(word for word in word_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before watching this . < br / > < br / > < br / > So Watch It Only If You Knew Susie ( Like Mimoso and Susana ) sounds like the ravings of a dance sequence where mannequins are used to better movies and compare it to just before tryouts and wore tights to cover his embezzlement . He played that role , making a supernatural psychological thriller worthy of some great actors were age-appropriate to this film is smart , moderately obnoxious skilled writer . But i would recommend this film , as in `` Fantastic Easter Special '' Sigmund & the lack of understanding to his bumbling , unsure-of-himself , low-key star , Janet Heffernan , Spence and Doug Pruzan ( Carrie 's boss , Dr. Janos Rukh ( Karloff ) demonstrates to colleagues Dr. Felix Benet ( Bela , Boris , a scientist . And while Olivier and Oberon are not familiar with one dull spot in this movie without having a torrid affair during the shootout ) , but there is a hardcore rock band from Des Moines , Iowa . Nine band members decide to help the Lumas find them in a long\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59Rrl0g31Vdn",
        "colab_type": "code",
        "outputId": "7d002aaa-c17a-4e35-c278-21ddfc2edd70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "word_list = model.generate(200, random_seed = -10)\n",
        "print(' '.join(word for word in word_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "in it . Although the society in Elizabethan England . Although this was MUCH better ! < br / > I must admit that « Kitchen » a « Tatiesque » movie ? Why come hero always have this re-make which , given Ferrari 's pieces are and what are told first from Sue 's Asian boyfriend but even i can identify with one big ball of cheese that surrounds you enabling you to < br / > < br / > The original animated Dark Knight returns in his life as portrayed in the kitchen scene , even with it '' . It 's a clever one . If you do n't know what watching this in his travel. < br / > God answers the citizens sing to welcome their new princess '' keep bumping into this film is a neo-Hollywood faux-liberal , so I ca n't be worse than even your average cinema . From the cutting dialogue to the brilliant character acting on the Lifetime network . An amateur film made in Afghanistan but i just had someone new , indefinable world beyond man 's name in his role really projecting Wilde himself , really ,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwN872ro1Zaz",
        "colab_type": "code",
        "outputId": "01cca837-121d-43da-9280-483489fe8568",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "word_list = model.generate(200, random_seed = 33)\n",
        "print(' '.join(word for word in word_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "important role of Lane.They have changed the course of the guys work there , waiting for a fool for buying it ? Have they changed the outfit and is very difficult for young talented girls . </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gg4IxOoV1cHA",
        "colab_type": "code",
        "outputId": "81a3fc33-a793-41df-dbd7-6d42b98470d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "word_list = model.generate(200, random_seed = 69)\n",
        "print(' '.join(word for word in word_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no attempt to portray an interpretive expression of a soldier trying to do some housework for her father 's accountant ( King ) from the set 's cocoon of cheese that surrounds it . </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsqJCX3pnmnx",
        "colab_type": "text"
      },
      "source": [
        "# Carculating perplexity for all reviews statistical modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8e1fsCmntWc",
        "colab_type": "code",
        "outputId": "325be765-e029-4fe4-bdbf-47baf957fb05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "test_set = len(generated_text.split())\n",
        "\n",
        "import random\n",
        "r = random.sample(range(test_set),15)\n",
        "test_set_sample = [generated_text.split()[i] for i in r]\n",
        "\n",
        "import collections, nltk\n",
        "# we first tokenize the text corpus\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "#here you construct the language model for creating a dictionary\n",
        "def unigram(tokens): \n",
        "  # creating a dictionary   \n",
        "    model1 = collections.defaultdict(lambda: 0.01) #For words outside the scope of its knowledge, it assigns a low probability of 0.01\n",
        "    for f in tokens:\n",
        "        try:\n",
        "            model1[f] += 1\n",
        "        except KeyError:\n",
        "            model1 [f] = 1\n",
        "            continue\n",
        "    N = float(sum(model1.values()))\n",
        "    for word in model1:\n",
        "        model1[word] = model1[word]/N\n",
        "    return model1\n",
        "\n",
        "#computes perplexity of the trigram model on a testset  \n",
        "def perplexity(testset, model1):\n",
        "    testset= nltk.word_tokenize(testset)\n",
        "    #testset = testset.split()\n",
        "    perplexity = 1\n",
        "    N = 3 #change values of N for calculating perplexity of bi - gram or other models\n",
        "  #calculating inverse probability of occurence of words\n",
        "    for word in testset:\n",
        "        N += 1\n",
        "        perplexity = perplexity * (1/model1[word])\n",
        "    perplexity = pow(perplexity, 1/float(N)) \n",
        "    return perplexity\n",
        "\n",
        "model1 = unigram(tokens)\n",
        "\n",
        "perplexities = []\n",
        "\n",
        "for i in range(len(test_set_sample)):\n",
        "    pp= perplexity(test_set_sample[i], model1)\n",
        "    perplexities.append(pp)\n",
        "print(\"perplexity is\", np.mean(pp))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "perplexity is 2.606965673054827\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}